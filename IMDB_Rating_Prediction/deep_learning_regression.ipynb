{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep_Learning_regression.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"13-q8udRurysHJHPdf2qnWkTu73l2Uze8","authorship_tag":"ABX9TyN0JIZ0pd8+Yy5+aBEAxc9n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JbXUuEWxakd_","colab_type":"code","outputId":"54da8fd5-c941-4762-9a9c-bfd33f30787f","executionInfo":{"status":"ok","timestamp":1585893841827,"user_tz":360,"elapsed":438,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import os\n","import shutil\n","import json\n","import re\n","import os\n","import datetime\n","from itertools import permutations \n","import numpy as np\n","import time\n","from matplotlib import pyplot as plt\n","from google.colab import drive\n","import imageio\n","import glob\n","from PIL import Image\n","from functools import reduce\n","import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder\n","import itertools\n","import pickle\n","import random\n","from sklearn.model_selection import train_test_split\n","from fastai.layers import *\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import gensim\n","from gensim.parsing.preprocessing import remove_stopwords\n","import nltk\n","nltk.download('punkt')\n","# from gensim.models import Word2Vec\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","from torchvision import transforms, utils, datasets\n","from tqdm import tqdm\n","from torch.nn.parameter import Parameter\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","data_path = \"./drive/My Drive/cs472/data/data.pkl\"\n","data_path2 = \"./drive/My Drive/cs472/data/data2.pkl\"\n","\n","clean_data_path = \"./drive/My Drive/cs472/data/clean_data.pkl\"\n","clean_data_path2 = \"./drive/My Drive/cs472/data/clean_data2.pkl\"\n","\n","word_dict_path = \"./drive/My Drive/cs472/data/word_dict.pkl\"\n","name_dict_path = \"./drive/My Drive/cs472/data/name_dict.pkl\"\n","\n","word_encoder_path = \"./drive/My Drive/cs472/models/word_encoder.pkl\"\n","name_encoder_path = \"./drive/My Drive/cs472/models/name_encoder.pkl\"\n","word_decoder_path = \"./drive/My Drive/cs472/models/word_decoder.pkl\"\n","name_decoder_path = \"./drive/My Drive/cs472/models/name_decoder.pkl\"\n","\n","regressor_path = \"./drive/My Drive/cs472/models/regressor.pkl\"\n","regressor_path2 = \"./drive/My Drive/cs472/models/regressor2.pkl\""],"execution_count":86,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eHkGjZWY_loy","colab_type":"text"},"source":["# Preprocess"]},{"cell_type":"code","metadata":{"id":"_jduU4UWuMbp","colab_type":"code","colab":{}},"source":["def preprocess(file_path):\n","\n","    d = pd.read_pickle(file_path)\n","\n","    # get names\n","    d['actor'] = d['actor'].str.lower().values\n","    d['director'] = d['director'].str.lower().values\n","    names = list(d['actor'].values) + list(d['director'].values)\n","    names = list(dict.fromkeys(names))\n","\n","    max_name_length = len(max(max(d['actor'], key=len), max(d['director'], key=len)))\n","    print(\"Max length of names:\", max_name_length)\n","\n","    name_dict = dict(map(reversed, enumerate(names)))\n","    longest_name = max(names, key=len)\n","    len_longest_name = len(longest_name)\n","    name_embedding_length = len_longest_name - (len_longest_name//3)\n","\n","    name_dict[len(name_dict)] =  \"SOS\"\n","    name_dict[len(name_dict)] =  \"EOS\"\n","\n","    name_dict['em_length'] = name_embedding_length\n","    name_dict['max_length'] = max_name_length\n","\n","    print(name_embedding_length)\n","    print(len(name_dict))\n","\n","    # save out dictionary\n","    f = open(name_dict_path,\"wb\")\n","    pickle.dump(name_dict,f)\n","    f.close()\n","\n","    # titles and descriptions\n","    d['title'] = d.title.str.lower()\n","    d['title'] = d.title.str.replace(\"-\", \" \").copy()\n","    title_tokens = set(itertools.chain(*np.array(d.title.apply(word_tokenize).values)))\n","        \n","    d['description'] = d.description.str.replace(\"-\", \" \").copy()\n","    d['description'] = d.description.str.lower().copy()\n","    d['description'] = d.description.apply(remove_stopwords).copy()\n","    d['description'] = d.description.str.replace('[^\\w\\s]','').copy()\n","    tokens = set(itertools.chain(*np.array(d.description.apply(word_tokenize).values)))\n","\n","    # save cleaned pickle file\n","    pd.to_pickle(d, clean_data_path2)\n","\n","    # get the vocabulary\n","    s = tokens.union(title_tokens)\n","    vocab_size = len(s)\n","\n","    # make a dictionary of all the words and save it out\n","    word_dict = dict(map(reversed, enumerate(s)))\n","\n","    # get longest word\n","    longest_word = max(s, key=len)\n","    len_longest_word = len(longest_word)\n","    embedding_length = len_longest_word - (len_longest_word//3)\n","\n","    max_desc_length = len(max(tokens, key=len))\n","    print(\"Max length of descriptions:\", max_desc_length)\n","\n","    word_dict[len(name_dict)] =  \"SOS\"\n","    word_dict[len(name_dict)] =  \"EOS\"\n","\n","    word_dict['em_length'] = embedding_length\n","    word_dict['max_length'] = max_desc_length\n","\n","\n","    # save out dictionary\n","    f = open(word_dict_path,\"wb\")\n","    pickle.dump(word_dict,f)\n","    f.close()\n","    \n","    print(\"Size of Vocabulary:\", vocab_size)\n","    print(\"Longest Word:\", longest_word, \"with length\", len_longest_word)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RI7bvsqnuvCo","colab_type":"code","outputId":"e8dce095-c9a9-40fb-d0e8-95e08e1e7a64","executionInfo":{"status":"ok","timestamp":1585594188941,"user_tz":360,"elapsed":8421,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["%time preprocess(data_path2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Max length of names: 30\n","24\n","19471\n","Max length of descriptions: 27\n","Size of Vocabulary: 34209\n","Longest Word: romanticpoliticalbiographic with length 27\n","CPU times: user 6.26 s, sys: 72.2 ms, total: 6.33 s\n","Wall time: 7.71 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w2LpMd7ivyPU","colab_type":"code","outputId":"6859196c-7a6c-44ac-adf6-b7ca75fe1674","executionInfo":{"status":"ok","timestamp":1585594188951,"user_tz":360,"elapsed":7779,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["data = pd.read_pickle(clean_data_path2)\n","# len(data[data.genre==1])\n","len(word_tokenize(data.title.max()))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"OWMzoJcw_owH","colab_type":"text"},"source":["# Dataset Loader"]},{"cell_type":"code","metadata":{"id":"hJ-K4HRAa4aE","colab_type":"code","colab":{}},"source":["class MovieRatings(Dataset):\n","\n","    def __init__(self, word_encoder, name_encoder, file_path=clean_data_path2, test=False, test_split=.3,classifier=False):\n","        \"\"\"\n","        params:\n","            genre int: 1-14 corrisponding to just training \n","            on a certain genre or not\n","        \"\"\"\n","        super(MovieRatings, self).__init__()\n","\n","        self.classifier = classifier\n","        self.test = test\n","\n","        self.word_embeds, self.word_dict, self.word_embed_len, self.word_max_length  = self._get_embedding(word_dict_path)\n","        self.name_embeds, self.name_dict, self.name_embed_len, self.name_max_length = self._get_embedding(name_dict_path)\n","\n","        data = pd.read_pickle(file_path)\n","        s_rating_dummies = pd.get_dummies(data.s_rating, drop_first=True)\n","        genre_dummies = pd.get_dummies(data.genre, drop_first=True)\n","\n","        n = int(len(data)*test_split)\n","\n","        self.test_data = data.iloc[:n,:]\n","        self.test_s_rating_dummies = s_rating_dummies.iloc[:n,:]\n","        self.test_genre_dummies = genre_dummies.iloc[:n,:]\n","\n","        self.train_data = data.iloc[n:,:]\n","        self.train_s_rating_dummies = s_rating_dummies.iloc[n:,:]\n","        self.train_genre_dummies = genre_dummies.iloc[n:,:]\n","\n","        self.word_encoder = word_encoder \n","        self.name_encoder = name_encoder\n","\n","\n","    def _get_embedding(self,path_):\n","\n","        # get name embedding space\n","        dict_ = pickle.load( open(path_, \"rb\" ) ) \n","\n","        # make embedding space for names\n","        vocab_size = len(dict_) - 2\n","        \n","        embed_length = dict_['em_length']\n","        return nn.Embedding(vocab_size, embed_length), dict_, embed_length, dict_['max_length']\n","\n","    def __getitem__(self, i):\n","        if self.test:\n","            data = self.test_data\n","            s_rating_dummies = self.test_s_rating_dummies\n","            genre_dummies = self.test_genre_dummies\n","        else:\n","            data = self.train_data\n","            s_rating_dummies = self.train_s_rating_dummies\n","            genre_dummies = self.train_genre_dummies\n","\n","        actor = data.actor.iloc[i].lower()\n","        director = data.director.iloc[i].lower()\n","\n","        title_tokens = word_tokenize(data.title.iloc[i].lower())\n","        text_tokens = word_tokenize(data.description.iloc[i].lower())\n","\n","        word_idx = torch.tensor([],dtype=torch.long)\n","\n","        for t in text_tokens + title_tokens:\n","            idx = torch.tensor([self.word_dict[t]])\n","            word_idx = torch.cat((word_idx, idx),0)\n","        \n","        actor_idx = torch.tensor([self.name_dict[actor]], dtype=torch.long)\n","        director_idx = torch.tensor([self.name_dict[director]],dtype=torch.long)\n","\n","        name_idx = torch.cat((actor_idx, director_idx),0)\n","\n","        run_time = torch.Tensor([data.runtime.iloc[i]]).cuda()\n","        s_rating = torch.Tensor(s_rating_dummies.iloc[i].values).cuda()\n","        genre = torch.Tensor(genre_dummies.iloc[i].values).cuda()\n","\n","        if self.classifier:\n","            label = float(data.rating_class.iloc[i])\n","        else:\n","            label = float(data.rating_label.iloc[i])\n","\n","        w_encoder_hidden = self.word_encoder.initHidden()\n","        n_encoder_hidden = self.name_encoder.initHidden()\n","\n","        # encode description, title and names \n","        for e_i in range(len(word_idx)):\n","            encoder_output, w_encoder_hidden = self.word_encoder(word_idx[e_i].cuda(), w_encoder_hidden)\n","\n","        for e_i in range(len(name_idx)):\n","            encoder_output, n_encoder_hidden = self.name_encoder(name_idx[e_i].cuda(), n_encoder_hidden)\n","\n","        w_encoder_hidden = w_encoder_hidden.squeeze(0).squeeze(0)\n","        n_encoder_hidden = n_encoder_hidden.squeeze(0).squeeze(0)\n","\n","        return torch.cat((w_encoder_hidden,n_encoder_hidden, s_rating, run_time, genre), dim=0), label\n","\n","    def __len__(self):\n","        if self.test:\n","            return len(self.test_data)\n","        else:\n","            return len(self.train_data)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rSIzt1qaGjZ","colab_type":"code","outputId":"07ac248e-07d7-4252-8dc3-38b765c43191","executionInfo":{"status":"error","timestamp":1585872830208,"user_tz":360,"elapsed":729,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["dataset = MovieRatings()\n","# len(dataset.data)\n","dataset[0]\n","# data_loader = DataLoader(dataset)"],"execution_count":25,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-8c2758620c25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMovieRatings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# len(dataset.data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# data_loader = DataLoader(dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'word_encoder' and 'name_encoder'"]}]},{"cell_type":"markdown","metadata":{"id":"mFvVJi1T_r_-","colab_type":"text"},"source":["# Encoder/ Decoder Netowrks For Text Data"]},{"cell_type":"code","metadata":{"id":"C2NlsHpknjSM","colab_type":"code","colab":{}},"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","    def forward(self, input_, hidden):\n","        embedded = self.embedding(input_).view(1, 1, -1)\n","        output = embedded\n","        output, hidden = self.gru(output, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ctF-_22ZY6-f","colab_type":"code","colab":{}},"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input_, hidden):\n","        output = self.embedding(input_).view(1, 1, -1)\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","        output = self.softmax(self.out(output[0]))\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EBe4U9ZQ_wv9","colab_type":"text"},"source":["# Training Encoder/Decoder"]},{"cell_type":"code","metadata":{"id":"m8maNM2YIkTr","colab_type":"code","colab":{}},"source":["teacher_forcing_ratio = 0.5\n","\n","\n","def train(input_tensor, encoder, decoder, SOS_token, EOS_token,\n","          encoder_optimizer, decoder_optimizer, \n","          criterion, max_length=50):\n","    \n","    \n","    encoder_hidden = encoder.initHidden()\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = input_tensor.size(0)\n","    target_length = input_length\n","    target_tensor = input_tensor.clone()\n","\n","    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","    loss = 0\n","\n","    # loop through input to encode\n","    for e_i in range(input_length):\n","        encoder_output, encoder_hidden = encoder(input_tensor[e_i], encoder_hidden)\n","        encoder_outputs[e_i] = encoder_output[0, 0]\n","\n","    decoder_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n","    decoder_hidden = encoder_hidden\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","    print(\"target\", target_tensor)\n","\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for d_i in range(target_length):\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","            print(\"decode output\", decoder_output)\n","            loss += criterion(decoder_output, target_tensor[d_i:d_i+1])\n","            decoder_input = target_tensor[d_i]\n","\n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        for d_i in range(target_length):\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","            print(\"decode output\", decoder_output)\n","            loss += criterion(decoder_output, target_tensor[d_i:d_i+1])\n","            if decoder_input.item() == EOS_token:\n","                break\n","\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1OTogaUDOYVD","colab_type":"code","colab":{}},"source":["def trainIters(encoder, decoder, data_set, epochs, kind=\"word\", print_every=1000, plot_every=1000, learning_rate=0.01):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    data_loader = DataLoader(MovieRatings(), batch_size=1, shuffle=True)\n","\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","\n","    SOS_token = len(data_set.word_dict)-4 if kind == \"word\" else len(data_set.name_dict)-4\n","    EOS_token = len(data_set.word_dict)-3 if kind == \"word\" else len(data_set.name_dict)-3\n","    \n","    criterion = nn.NLLLoss()\n","    print(\"training\")\n","\n","    for e in range(epochs):\n","        print(\"epoch\",e)\n","        for i, (word_idx, name_idx, numeric, label) in enumerate(data_loader):\n","            if kind == \"word\":\n","                input_tensor = word_idx.squeeze(0)\n","            else:\n","                input_tensor = name_idx.squeeze(0)\n","\n","            loss = train(input_tensor, encoder, decoder, SOS_token,EOS_token,\n","                        encoder_optimizer, decoder_optimizer, criterion)       \n","        \n","            print_loss_total += loss\n","            plot_loss_total += loss\n","\n","            if i % print_every == 0:\n","                print_loss_avg = print_loss_total / print_every\n","                print_loss_total = 0\n","                print(print_loss_avg)\n","\n","            if i % plot_every == 0:\n","                plot_loss_avg = plot_loss_total / plot_every\n","                plot_losses.append(plot_loss_avg)\n","                plot_loss_total = 0\n","\n","        if kind == \"word\":\n","            torch.save(encoder, word_encoder_path)\n","            torch.save(decoder, word_decoder_path)\n","        else:\n","            torch.save(encoder, name_encoder_path)\n","            torch.save(encoder, name_decoder_path)\n","\n","    return encoder"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFO-9_W8U2kD","colab_type":"code","colab":{}},"source":["def main(kind=\"word\"):\n","\n","    dataset = MovieRatings()\n","\n","    if kind == \"word\":\n","        vocab_size = len(dataset.word_dict)\n","        embed_size = dataset.word_embed_len\n","\n","        if os.path.exists(word_encoder_path):\n","            encoder = torch.load(word_encoder_path)\n","            decoder = torch.load(word_decoder_path)\n","\n","            print(\"loaded\")\n","        else:\n","            encoder = EncoderRNN(vocab_size,embed_size).to(device)\n","            decoder = DecoderRNN(embed_size, vocab_size).to(device)\n","\n","\n","    elif kind == \"name\":\n","        vocab_size = len(dataset.name_dict)\n","        embed_size = dataset.name_embed_len\n","\n","        if os.path.exists(name_encoder_path):\n","            encoder = torch.load(name_encoder_path)\n","            decoder = DecoderRNN(embed_size, vocab_size).to(device)\n","            torch.save(encoder, name_decoder_path)\n","            decoder = torch.load(name_decoder_path)\n","        else:\n","            encoder = EncoderRNN(vocab_size,embed_size).to(device)\n","            decoder = DecoderRNN(embed_size, vocab_size).to(device)\n","\n","    encoder = trainIters(encoder, decoder, dataset, 20, kind=kind)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ChTJ2yi3Xpvk","colab_type":"code","outputId":"84963c29-380d-4432-bcce-dfc474f5e026","executionInfo":{"status":"error","timestamp":1585200287148,"user_tz":360,"elapsed":480,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["main()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["loaded\n","training\n","epoch 0\n","0.011638392130533854\n","10.373293525701\n","9.635096962202237\n","9.33704219610897\n","9.155613743206626\n","9.067165618168616\n","9.003137619822688\n","8.91562369269399\n","8.885386338250507\n","8.898630150996302\n","8.843544294167538\n","8.830918420940758\n","8.80016822998259\n","8.752269705635646\n","8.75423780210036\n","8.744448358876495\n","8.750338638753915\n","8.69160266990502\n","8.712438485496165\n","8.749938103240646\n","8.683691126148828\n","8.72991367904921\n","8.696759021358348\n","8.687068909202722\n","8.68275417005081\n","8.642807207172668\n","8.659069039481912\n","8.631738657001295\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type DecoderRNN. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["epoch 1\n","0.5466761194003443\n","8.603201221325639\n","8.630741364553426\n","8.612298289006374\n","8.626766278613335\n","8.596503582112225\n","8.604612368337008\n","8.619568051557232\n","8.582622720719337\n","8.599156915624413\n","8.608003263888124\n","8.627156635675195\n","8.597055373085029\n","8.619668791375942\n","8.570115033335178\n","8.625201493831335\n","8.602384558451329\n","8.574679501888825\n","8.590454130227817\n","8.558109694075577\n","8.59649586463335\n","8.53896045147026\n","8.570470656359175\n","8.56527411858523\n","8.574539609221478\n","8.562724701879112\n","8.566540567480978\n","8.56741044898803\n","epoch 2\n","0.5482652906212904\n","8.502424294157839\n","8.52661898396534\n","8.538345995428326\n","8.499856123571755\n","8.517296507530132\n","8.494928444816324\n","8.469397302003276\n","8.504019129983115\n","8.536309990973269\n","8.49248158728073\n","8.449360893000698\n","8.47493880804904\n","8.512396106619912\n","8.49282759631716\n","8.458258771325074\n","8.456735762711354\n","8.466036373620927\n","8.475839800901067\n","8.453195493395457\n","8.45532513921198\n","8.51905865078503\n","8.50744433186379\n","8.471606060112968\n","8.483179283040236\n","8.482717937972259\n","8.441590240899819\n","8.457529271706207\n","epoch 3\n","0.5414279545770946\n","8.41022794735085\n","8.429225241644312\n","8.442732090026066\n","8.358630943876177\n","8.37755438944821\n","8.404644257928089\n","8.397843153707358\n","8.38824158611372\n","8.432711959222987\n","8.459300661772202\n","8.424146355703579\n","8.48271622059074\n","8.444768915182745\n","8.418418080403082\n","8.424037304256228\n","8.414554422051868\n","8.411791387340475\n","8.401490922595567\n","8.378381373204626\n","8.35953878996762\n","8.47899455503687\n","8.398768752357093\n","8.383759622392317\n","8.395963757906806\n","8.4035463508412\n","8.412780103715368\n","8.353206346897629\n","epoch 4\n","0.5264059826405036\n","8.337736603804762\n","8.352825485536819\n","8.327097275884162\n","8.383797433244244\n","8.38386816036647\n","8.31506272636069\n","8.36690997103328\n","8.337886802769278\n","8.352636795697128\n","8.348886927927694\n","8.337717544602533\n","8.354870645414833\n","8.397617956657639\n","8.335632060513898\n","8.38323585911559\n","8.346226016505403\n","8.350422862357744\n","8.325089460809124\n","8.374760507185199\n","8.341543488099768\n","8.303564470066137\n","8.361417826155053\n","8.338695602245314\n","8.3043827282768\n","8.359429353574562\n","8.372674012714706\n","8.346506592265369\n","epoch 5\n","0.5291960903282562\n","8.295229270187544\n","8.26825999934272\n","8.312769262788404\n","8.327966508570059\n","8.288580030400313\n","8.313544273991134\n","8.252293599776849\n","8.311338980057139\n","8.296345221295836\n","8.30532663953868\n","8.3195802766603\n","8.277037939177863\n","8.308130817325297\n","8.354514147639726\n","8.28427162436432\n","8.272606789198603\n","8.287522724208925\n","8.278140545052809\n","8.305436261844958\n","8.250520456952023\n","8.248394152029048\n","8.293756274944647\n","8.334216711090475\n","8.249334030896836\n","8.299315828046572\n","8.340276901603582\n","8.255340163912539\n","epoch 6\n","0.5318947577390285\n","8.244753421251092\n","8.214772288692595\n","8.262471195901654\n","8.229568368229604\n","8.261701504339042\n","8.27162477739527\n","8.270345469500784\n","8.261130844415513\n","8.278880363175347\n","8.227822298337168\n","8.245282975318117\n","8.25196640924808\n","8.222578546811976\n","8.269966410223164\n","8.241784459075138\n","8.240468648641267\n","8.231121752665635\n","8.293213053994492\n","8.22525111939148\n","8.260916059130093\n","8.221241340434297\n","8.249960493099648\n","8.215450916329884\n","8.21910937941657\n","8.222021972328362\n","8.237490533840226\n","8.229863641299593\n","epoch 7\n","0.5296088004841267\n","8.201258885926864\n","8.181827385363947\n","8.199590868612141\n","8.197015644480917\n","8.217056963767966\n","8.240309770570976\n","8.188313761093518\n","8.2002954337333\n","8.203385001999086\n","8.180920734684504\n","8.225272140915738\n","8.203353272688613\n","8.2199603717183\n","8.225718118782265\n","8.200134245949625\n","8.159773542817007\n","8.161658123224637\n","8.21255699907186\n","8.187898911683934\n","8.182890027220987\n","8.141668625399438\n","8.227842618291778\n","8.15605096805415\n","8.18242240111208\n","8.175694006071257\n","8.179186804687742\n","8.183585236249868\n","epoch 8\n","0.5195730553735509\n","8.122491742867807\n","8.127313826651514\n","8.16493405508743\n","8.144784341119829\n","8.160098772286018\n","8.119732772699486\n","8.17755109128125\n","8.184902777781849\n","8.134362823559538\n","8.169342376783677\n","8.136101643450239\n","8.151206361461782\n","8.133990957649615\n","8.181215761847414\n","8.185114273673385\n","8.114013928662946\n","8.173676993738995\n","8.132046321350295\n","8.145714088864088\n","8.088793561790466\n","8.096021153073014\n","8.141423925316875\n","8.10145648537259\n","8.173715982055764\n","8.135566450077068\n","8.127665394715507\n","8.164234796601997\n","epoch 9\n","0.5201742275801013\n","8.121954067738942\n","8.078918876182566\n","8.082255278511353\n","8.069644951995159\n","8.097616488370965\n","8.111972757309413\n","8.097200065050945\n","8.113939066605143\n","8.111863390640458\n","8.137632887295533\n","8.117796573686293\n","8.09592696580521\n","8.107990872976565\n","8.120256780790358\n","8.086455251936949\n","8.110958583652687\n","8.118376831760637\n","8.11552750172442\n","8.102968361791646\n","8.081113338529454\n","8.07321792038741\n","8.116056159273224\n","8.121025409645764\n","8.065811535123\n","8.129228036650295\n","8.033870579285994\n","8.111521593373798\n","epoch 10\n","0.5297200209052254\n","8.040741924341779\n","8.034136281399384\n","8.046519647361217\n","8.086966449744471\n","8.071575424294432\n","8.054004254714727\n","8.063344973385771\n","8.0706029942313\n","8.084307664850853\n","8.070079951088125\n","8.06900237851111\n","8.062374171646908\n","8.081254168114809\n","8.0605854950039\n","8.048158182230328\n","8.06418103460473\n","8.014681335023843\n","8.128182477197015\n","8.073614975591134\n","8.060181930537098\n","8.040672055387555\n","8.0093858788283\n","8.034067158259413\n","8.11414222726558\n","8.104865863312614\n","8.076365804470807\n","8.06392072136463\n","epoch 11\n","0.5129957022591355\n","8.032164526243555\n","7.974416171665048\n","8.061911945498391\n","7.98674059079834\n","7.998579310478422\n","8.018072971995158\n","8.013414869855044\n","8.042921952154073\n","8.08966954227985\n","8.009488650178156\n","8.022875150043967\n","8.013118100924652\n","8.08774058828915\n","8.020759805553801\n","8.030457091574743\n","8.013052133829058\n","7.99188809321796\n","8.048924350639243\n","8.029672276292299\n","8.001052522653149\n","8.047843321413296\n","8.033823835110766\n","8.022814094731268\n","8.077277114981205\n","8.022042085931835\n","8.005625408377496\n","7.986594056291389\n","epoch 12\n","0.5122216601066721\n","8.044067215522592\n","8.004981771014696\n","7.97615725522374\n","7.996826999740463\n","8.012214392066525\n","7.997670513563496\n","7.99895447927743\n","8.005258006524832\n","7.9866083864050035\n","7.990736995739726\n","7.993916169346338\n","7.992817360721867\n","7.979224895899714\n","8.000964265390875\n","8.03008169985294\n","7.999943558167807\n","7.968315974061014\n","7.96731029619016\n","7.9646265094382676\n","7.950190375554507\n","7.955795625579568\n","8.018544884471304\n","7.939409032062948\n","7.980326929299012\n","7.999299198761284\n","7.972492219046895\n","7.994241981733742\n","epoch 13\n","0.5083340961043722\n","7.967395306785908\n","7.957671338546199\n","7.968034060146203\n","7.926698885076311\n","7.942758214689361\n","7.986601399716496\n","7.990722284099343\n","7.974962463575636\n","7.92215924523236\n","7.932515881830585\n","7.947643600640009\n","7.99389772372788\n","7.917300542266247\n","7.95641910258336\n","7.978385294055811\n","7.934040992175599\n","7.956111252290353\n","7.92315660474688\n","7.9899587927499605\n","7.980692378641307\n","7.926940015944461\n","7.968348224399211\n","7.925970660524802\n","7.913298097738542\n","7.9213367274471995\n","7.968301384091551\n","7.949488973694716\n","epoch 14\n","0.5094760734315587\n","7.868759605476889\n","7.889956235334435\n","7.934994170224864\n","7.952348623857414\n","7.907350405736766\n","7.941536627776966\n","7.905665439876907\n","7.928237608522907\n","7.8971869596880016\n","7.938366661887006\n","7.88592795674837\n","7.918999607988047\n","7.898295109785202\n","7.946524973608819\n","7.912509163497471\n","7.943225372693353\n","7.910448778998141\n","7.920632439265352\n","7.904473244385885\n","7.935705855240993\n","7.93987174290834\n","7.978750730992345\n","7.867380528374305\n","7.909359023534839\n","7.891913342791608\n","7.925742920897303\n","7.9306617331866835\n","epoch 15\n","0.5083950100067194\n","7.867078596021109\n","7.876095241663489\n","7.901498799299375\n","7.8754181511928785\n","7.8735516796590375\n","7.879675495570183\n","7.899486154710799\n","7.858229652837348\n","7.8656939994061545\n","7.8643363738702705\n","7.883400224306346\n","7.889493409499808\n","7.899057346458653\n","7.836128855035439\n","7.894253791293938\n","7.836239357511029\n","7.892070199196661\n","7.894060541394181\n","7.9156461951480726\n","7.902802396151462\n","7.875155437242792\n","7.931637789218301\n","7.912793734808367\n","7.854241366525114\n","7.933325751121856\n","7.89908827138946\n","7.912560906482355\n","epoch 16\n","0.5118676919911237\n","7.838965920131665\n","7.841227383078305\n","7.831524495745705\n","7.846168522329449\n","7.833910773758894\n","7.847492126682529\n","7.874432237852573\n","7.819489650511144\n","7.858073244007753\n","7.879913539541701\n","7.907211261130017\n","7.849578782991924\n","7.896388461940929\n","7.856448066919582\n","7.873162524063869\n","7.8637956508183455\n","7.824056793333757\n","7.911977034152787\n","7.793987450057694\n","7.916049033461044\n","7.805235543205747\n","7.858276032572513\n","7.879993667975673\n","7.817981429334081\n","7.848881514999432\n","7.909911107574352\n","7.84992209602066\n","epoch 17\n","0.4911605985575471\n","7.744450139198631\n","7.790488443257362\n","7.826441987990939\n","7.840601705341012\n","7.850153403065103\n","7.824302328745366\n","7.85479925619545\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WsaA9ydIlxY4","colab_type":"code","colab":{}},"source":["main(kind=\"name\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KkCiBG025OY3","colab_type":"code","colab":{}},"source":["def eval(encoder, decoder, input_tensor, dict_, inv_dict, max_length=50):\n","\n","    for word in input_tensor:\n","        print(inv_dict[word])\n","\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(input_lang, sentence)\n","        encoder_hidden = encoder.initHidden()\n","\n","        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","        for ei in range(input_length):\n","            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n","                                                     encoder_hidden)\n","            encoder_outputs[ei] += encoder_output[0, 0]\n","\n","        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n","\n","        decoder_hidden = encoder_hidden\n","\n","        decoded_words = []\n","        decoder_attentions = torch.zeros(max_length, max_length)\n","\n","        for di in range(max_length):\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","            decoded_words.append(decoder_output)\n","\n","    for word in decoded_words:\n","        print(inv_dict[word])\n","\n","    return decoded_words, decoder_attentions[:di + 1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QX_OIeO5UCi","colab_type":"code","colab":{}},"source":["def evaluate(n=1):\n","\n","    encoder = torch.load(word_encoder_path)\n","    decoder = torch.load(word_decoder_path)\n","    data_loader = DataLoader(MovieRatings(), batch_size=1, shuffle=True)\n","\n","    dict_ = pickle.load( open(word_dict_path, \"rb\" ) ) \n","    inv_dict = dict(map(reversed, dict_.items()))\n","\n","    for i in range(n):\n","        eval(encoder, decoder, data_loader[i], dict_, inv_dict)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e2wrdjtvIaib","colab_type":"text"},"source":["# Regression Neural Net"]},{"cell_type":"code","metadata":{"id":"fyOTe3JMVY-Q","colab_type":"code","colab":{}},"source":["class RegressionNet(nn.Module):\n","\n","    def __init__(self, in_dim=64):\n","        super(RegressionNet, self).__init__()\n","\n","        # another way to define a network\n","        self.net = torch.nn.Sequential(\n","                        torch.nn.Linear(in_dim, 100),\n","                        torch.nn.LeakyReLU(),\n","                        torch.nn.Linear(100, 200),\n","                        torch.nn.LeakyReLU(),\n","                        torch.nn.Linear(200, 100),\n","                        torch.nn.LeakyReLU(),\n","                        torch.nn.Linear(100, 50),\n","                        torch.nn.LeakyReLU(),\n","                        torch.nn.Linear(50, 1))\n","\n","    def forward(self, x):\n","        return self.net(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CD_3N1AksAf1","colab_type":"code","colab":{}},"source":["def train_regress(epochs=20, print_every=10, r_path=regressor_path):\n","\n","    word_encoder = torch.load(word_encoder_path).cuda()\n","    name_encoder = torch.load(name_encoder_path).cuda()\n","    regressor = torch.load(r_path).cuda()\n","\n","    data_set = MovieRatings(word_encoder,name_encoder)\n","    data_loader = DataLoader(data_set, batch_size=64, shuffle=True)\n","    n = len(data_loader)\n","\n","    optimizer = torch.optim.Adam(itertools.chain(word_encoder.parameters(),\n","                                                 name_encoder.parameters(),\n","                                                 regressor.parameters()), lr=0.01)\n","    loss_func = MSELossFlat() #torch.nn.MSELoss()\n","    \n","\n","    losses = []\n","    for e in range(1,epochs+1):\n","        print(\"Epoch\", e)\n","        epoch_loss = 0\n","        print_loss_total = 0\n","\n","        for i, (x, label) in enumerate(data_loader):\n","            \n","            # run regression and get loss\n","            pred_y = regressor(x.cuda())\n","\n","            loss = loss_func(pred_y, label.long().cuda())\n","\n","            epoch_loss += loss.item()\n","            losses.append(loss.item())\n","\n","            if i % print_every == 0:\n","                print(\"Step\", i, \":\", np.mean(losses[-10:]))\n","                \n","                torch.save(regressor, r_path)\n","                torch.save(word_encoder, word_encoder_path)\n","                torch.save(name_encoder, name_encoder_path)\n","\n","            optimizer.zero_grad()   \n","            loss.backward()#retain_graph=True\n","            optimizer.step()\n","\n","        print(\"Average Epoch Loss:\", epoch_loss/n)\n","\n","    return data_set, regressor"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeGB4vy16zYU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"ceb1440a-4127-432c-bc69-f1764ba869e0","executionInfo":{"status":"ok","timestamp":1585886469755,"user_tz":360,"elapsed":4638316,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}}},"source":["torch.save(RegressionNet(), regressor_path2)\n","dataset, regressor = train_regress(r_path=regressor_path2)"],"execution_count":70,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type RegressionNet. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1\n","Step 0 : 29.97827911376953\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type RegressionNet. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Step 10 : 23.63948760032654\n","Step 20 : 3.7310128688812254\n","Step 30 : 3.5388919830322267\n","Step 40 : 3.538793110847473\n","Step 50 : 2.7908554077148438\n","Step 60 : 2.5297589182853697\n","Step 70 : 2.3980841398239137\n","Step 80 : 2.4206475734710695\n","Step 90 : 2.2256847381591798\n","Step 100 : 1.8266412615776062\n","Step 110 : 1.8226126432418823\n","Step 120 : 1.5747804522514344\n","Step 130 : 1.7296615242958069\n","Step 140 : 1.8519200921058654\n","Step 150 : 1.6441534876823425\n","Step 160 : 1.5568920850753785\n","Step 170 : 1.9605435609817505\n","Step 180 : 1.5984519720077515\n","Step 190 : 1.6057345032691956\n","Step 200 : 1.6863449811935425\n","Step 210 : 1.6038021206855775\n","Step 220 : 1.6635065078735352\n","Step 230 : 1.8346874356269836\n","Step 240 : 1.7405985355377198\n","Step 250 : 1.9457938551902771\n","Step 260 : 1.9671810150146485\n","Step 270 : 1.8160136222839356\n","Step 280 : 1.6802731275558471\n","Step 290 : 1.6670244097709657\n","Average Epoch Loss: 2.8833399986979935\n","Epoch 2\n","Step 0 : 1.6649189829826354\n","Step 10 : 1.9788997769355774\n","Step 20 : 2.287485146522522\n","Step 30 : 1.7207539796829223\n","Step 40 : 1.652290689945221\n","Step 50 : 1.8845996379852294\n","Step 60 : 2.249750077724457\n","Step 70 : 1.9487884759902954\n","Step 80 : 1.6277625560760498\n","Step 90 : 1.6056967735290528\n","Step 100 : 1.6372004985809325\n","Step 110 : 1.5774993181228638\n","Step 120 : 1.5866549611091614\n","Step 130 : 1.4299665451049806\n","Step 140 : 1.5778466820716859\n","Step 150 : 1.5760279297828674\n","Step 160 : 1.5161873698234558\n","Step 170 : 1.4124821543693542\n","Step 180 : 1.6462961554527282\n","Step 190 : 1.7293179988861085\n","Step 200 : 1.509164822101593\n","Step 210 : 1.4679332613945006\n","Step 220 : 1.5912979245185852\n","Step 230 : 1.567425000667572\n","Step 240 : 1.5821929752826691\n","Step 250 : 1.5926974654197692\n","Step 260 : 1.3974286437034606\n","Step 270 : 1.5099110722541809\n","Step 280 : 1.5787468671798706\n","Step 290 : 1.6186208963394164\n","Average Epoch Loss: 1.64872402445686\n","Epoch 3\n","Step 0 : 1.3620702657848596\n","Step 10 : 1.2463204741477967\n","Step 20 : 1.4407684445381164\n","Step 30 : 1.5221697807312011\n","Step 40 : 1.4963760495185852\n","Step 50 : 1.509787654876709\n","Step 60 : 1.8561355948448182\n","Step 70 : 1.7647417783737183\n","Step 80 : 1.5217341184616089\n","Step 90 : 1.6890993595123291\n","Step 100 : 1.4868379592895509\n","Step 110 : 1.3642069697380066\n","Step 120 : 1.5997826218605042\n","Step 130 : 1.4386527061462402\n","Step 140 : 1.4967490315437317\n","Step 150 : 1.7840389609336853\n","Step 160 : 1.527709698677063\n","Step 170 : 1.5122908353805542\n","Step 180 : 1.460020363330841\n","Step 190 : 1.5327269554138183\n","Step 200 : 1.7089971363544465\n","Step 210 : 1.529446816444397\n","Step 220 : 1.6548919558525086\n","Step 230 : 1.5933759450912475\n","Step 240 : 1.6371606111526489\n","Step 250 : 1.4974799513816834\n","Step 260 : 1.6630700469017028\n","Step 270 : 1.6147354543209076\n","Step 280 : 1.375999289751053\n","Step 290 : 1.5929417848587035\n","Average Epoch Loss: 1.550877813538317\n","Epoch 4\n","Step 0 : 1.504815047979355\n","Step 10 : 1.7267858862876893\n","Step 20 : 1.5673058032989502\n","Step 30 : 1.626545000076294\n","Step 40 : 1.3785046219825745\n","Step 50 : 1.3876992523670197\n","Step 60 : 1.4210498332977295\n","Step 70 : 1.6211077332496644\n","Step 80 : 1.3902597665786742\n","Step 90 : 1.611822748184204\n","Step 100 : 1.5364004373550415\n","Step 110 : 1.6773049354553222\n","Step 120 : 1.5308062791824342\n","Step 130 : 1.3861291885375977\n","Step 140 : 1.5128313899040222\n","Step 150 : 1.5571601510047912\n","Step 160 : 1.720938467979431\n","Step 170 : 1.5696717262268067\n","Step 180 : 1.5522677779197693\n","Step 190 : 1.6919808864593506\n","Step 200 : 1.540165913105011\n","Step 210 : 1.6423703789711\n","Step 220 : 1.4338365137577056\n","Step 230 : 1.6110373616218567\n","Step 240 : 1.6666245341300965\n","Step 250 : 1.595628809928894\n","Step 260 : 1.4882213652133942\n","Step 270 : 1.6022398114204406\n","Step 280 : 1.626602792739868\n","Step 290 : 1.443321907520294\n","Average Epoch Loss: 1.552909610612296\n","Epoch 5\n","Step 0 : 1.374821405520197\n","Step 10 : 1.6518424153327942\n","Step 20 : 1.349837863445282\n","Step 30 : 1.528708815574646\n","Step 40 : 1.591989815235138\n","Step 50 : 1.5234298825263977\n","Step 60 : 1.5604309558868408\n","Step 70 : 1.686996829509735\n","Step 80 : 1.5526540875434875\n","Step 90 : 1.7323722839355469\n","Step 100 : 1.456289291381836\n","Step 110 : 1.4458529949188232\n","Step 120 : 1.6670520186424256\n","Step 130 : 1.4820118427276612\n","Step 140 : 1.7638343095779419\n","Step 150 : 1.5394953846931458\n","Step 160 : 1.4708619594573975\n","Step 170 : 1.7182538986206055\n","Step 180 : 1.5155243098735809\n","Step 190 : 1.3711166501045227\n","Step 200 : 1.5004495322704314\n","Step 210 : 1.6547746062278748\n","Step 220 : 1.4915873885154725\n","Step 230 : 1.4776412010192872\n","Step 240 : 1.4976331114768981\n","Step 250 : 1.5526248574256898\n","Step 260 : 1.5113723039627076\n","Step 270 : 1.5231546759605408\n","Step 280 : 1.4457776546478271\n","Step 290 : 1.6298669815063476\n","Average Epoch Loss: 1.5444571895631476\n","Epoch 6\n","Step 0 : 1.7355843782424927\n","Step 10 : 37.219442343711854\n","Step 20 : 2.919375705718994\n","Step 30 : 2.3694878697395323\n","Step 40 : 1.8352251648902893\n","Step 50 : 2.6956708312034605\n","Step 60 : 2.655014765262604\n","Step 70 : 2.379216170310974\n","Step 80 : 16.528344917297364\n","Step 90 : 9201.722705984115\n","Step 100 : 28.28271942138672\n","Step 110 : 14.440769147872924\n","Step 120 : 12.338365411758422\n","Step 130 : 7.716919946670532\n","Step 140 : 2.915577471256256\n","Step 150 : 3.512195897102356\n","Step 160 : 2.7202182054519652\n","Step 170 : 2.321783256530762\n","Step 180 : 2.4433849096298217\n","Step 190 : 2.7967835903167724\n","Step 200 : 2.419164609909058\n","Step 210 : 2.52241907119751\n","Step 220 : 2.946675157546997\n","Step 230 : 2.7046121835708616\n","Step 240 : 2.6445175647735595\n","Step 250 : 3.0925604581832884\n","Step 260 : 3.7130346059799195\n","Step 270 : 2.766263222694397\n","Step 280 : 2.6533337354660036\n","Step 290 : 2.5109963536262514\n","Average Epoch Loss: 315.8066721897735\n","Epoch 7\n","Step 0 : 2.667663311958313\n","Step 10 : 3.6165145874023437\n","Step 20 : 3.7686266183853148\n","Step 30 : 2.953689897060394\n","Step 40 : 2.887112045288086\n","Step 50 : 2.9175302267074583\n","Step 60 : 2.1590128540992737\n","Step 70 : 2.102255177497864\n","Step 80 : 1.8610278487205505\n","Step 90 : 2.0883387327194214\n","Step 100 : 2.0658130764961244\n","Step 110 : 3.0008469104766844\n","Step 120 : 2.824829030036926\n","Step 130 : 3.0706108927726747\n","Step 140 : 2.700793731212616\n","Step 150 : 2.04626704454422\n","Step 160 : 2.282307356595993\n","Step 170 : 3.0106335997581484\n","Step 180 : 2.551390993595123\n","Step 190 : 2.2870928049087524\n","Step 200 : 1.6550434589385987\n","Step 210 : 1.7686383128166199\n","Step 220 : 1.7955866813659669\n","Step 230 : 1.9724652290344238\n","Step 240 : 1.8153830647468567\n","Step 250 : 1.6705814003944397\n","Step 260 : 1.8769320964813232\n","Step 270 : 2.32726628780365\n","Step 280 : 1.8890939354896545\n","Step 290 : 1.783400058746338\n","Average Epoch Loss: 2.390213683598772\n","Epoch 8\n","Step 0 : 2.5101970434188843\n","Step 10 : 4.130855226516724\n","Step 20 : 2.3215047478675843\n","Step 30 : 2.2333560824394225\n","Step 40 : 1.8047037720680237\n","Step 50 : 1.7877434372901917\n","Step 60 : 1.819242489337921\n","Step 70 : 1.6798553049564362\n","Step 80 : 1.8983316898345948\n","Step 90 : 1.6372491836547851\n","Step 100 : 1.5657285928726197\n","Step 110 : 1.7824323177337646\n","Step 120 : 1.7220169067382813\n","Step 130 : 1.6010552763938903\n","Step 140 : 1.769887101650238\n","Step 150 : 1.5856457829475403\n","Step 160 : 1.7303412675857544\n","Step 170 : 1.838882577419281\n","Step 180 : 1.7271790385246277\n","Step 190 : 1.7522998929023743\n","Step 200 : 1.6796557545661925\n","Step 210 : 1.4936877369880677\n","Step 220 : 1.5807045698165894\n","Step 230 : 1.8380472302436828\n","Step 240 : 1.6671354293823242\n","Step 250 : 1.3688917279243469\n","Step 260 : 1.5277010440826415\n","Step 270 : 1.590848183631897\n","Step 280 : 1.7210683941841125\n","Step 290 : 1.5882745146751405\n","Average Epoch Loss: 1.8103610936439398\n","Epoch 9\n","Step 0 : 1.4373624175786972\n","Step 10 : 1.6989512205123902\n","Step 20 : 1.60247243642807\n","Step 30 : 1.5623340129852294\n","Step 40 : 1.5208447217941283\n","Step 50 : 1.5443803071975708\n","Step 60 : 1.5816893100738525\n","Step 70 : 1.5711776733398437\n","Step 80 : 1.4454649567604065\n","Step 90 : 1.789099383354187\n","Step 100 : 1.7764584302902222\n","Step 110 : 1.4897143483161925\n","Step 120 : 1.4854795157909393\n","Step 130 : 1.5322001338005067\n","Step 140 : 1.7311649918556213\n","Step 150 : 1.5093348801136017\n","Step 160 : 2.106648087501526\n","Step 170 : 1.8912765026092528\n","Step 180 : 1.703181529045105\n","Step 190 : 1.7695202350616455\n","Step 200 : 1.7854139447212218\n","Step 210 : 1.6352163910865785\n","Step 220 : 1.6492251396179198\n","Step 230 : 1.798626220226288\n","Step 240 : 1.4799168705940247\n","Step 250 : 1.5728116393089295\n","Step 260 : 1.6875527322292327\n","Step 270 : 1.708535385131836\n","Step 280 : 1.6227306067943572\n","Step 290 : 1.7078002452850343\n","Average Epoch Loss: 1.645432257416473\n","Epoch 10\n","Step 0 : 1.4410066939890385\n","Step 10 : 1.910949945449829\n","Step 20 : 1.6095294594764709\n","Step 30 : 1.5855515718460083\n","Step 40 : 1.6887520551681519\n","Step 50 : 1.6368744015693664\n","Step 60 : 1.520090651512146\n","Step 70 : 1.635040295124054\n","Step 80 : 1.5030403971672057\n","Step 90 : 1.448222541809082\n","Step 100 : 1.5849572777748109\n","Step 110 : 1.555473017692566\n","Step 120 : 1.4871482610702516\n","Step 130 : 1.6298844695091248\n","Step 140 : 1.7599026679992675\n","Step 150 : 1.6785809516906738\n","Step 160 : 1.6104175090789794\n","Step 170 : 1.442113184928894\n","Step 180 : 1.741139256954193\n","Step 190 : 1.5580462217330933\n","Step 200 : 1.4894386649131774\n","Step 210 : 1.6667961120605468\n","Step 220 : 1.4518734693527222\n","Step 230 : 1.4925562739372253\n","Step 240 : 1.5075868487358093\n","Step 250 : 1.5116301774978638\n","Step 260 : 1.6438276410102843\n","Step 270 : 1.8112398862838746\n","Step 280 : 1.4602665305137634\n","Step 290 : 1.5610406875610352\n","Average Epoch Loss: 1.6024810115897696\n","Epoch 11\n","Step 0 : 1.9577155232429504\n","Step 10 : 2.332210457324982\n","Step 20 : 1.8155151724815368\n","Step 30 : 1.6752721548080445\n","Step 40 : 1.5336487770080567\n","Step 50 : 1.4800007820129395\n","Step 60 : 1.3039790272712708\n","Step 70 : 1.5628435492515564\n","Step 80 : 1.6641686677932739\n","Step 90 : 1.559764325618744\n","Step 100 : 1.7420625925064086\n","Step 110 : 1.5790706992149353\n","Step 120 : 1.6785444617271423\n","Step 130 : 1.7483733654022218\n","Step 140 : 1.653809404373169\n","Step 150 : 1.6358391046524048\n","Step 160 : 1.6674647092819215\n","Step 170 : 1.588201093673706\n","Step 180 : 1.4968189954757691\n","Step 190 : 1.457471239566803\n","Step 200 : 1.7276102304458618\n","Step 210 : 1.518505072593689\n","Step 220 : 1.5403757691383362\n","Step 230 : 1.4711898684501648\n","Step 240 : 1.5182250380516051\n","Step 250 : 1.772458589076996\n","Step 260 : 1.8027240872383117\n","Step 270 : 1.4997762322425843\n","Step 280 : 1.629825234413147\n","Step 290 : 1.5071747720241546\n","Average Epoch Loss: 1.6271177910594425\n","Epoch 12\n","Step 0 : 1.5109777241945266\n","Step 10 : 1.6794643640518188\n","Step 20 : 1.769541597366333\n","Step 30 : 1.6704235315322875\n","Step 40 : 1.5313626885414124\n","Step 50 : 1.867364728450775\n","Step 60 : 1.540811550617218\n","Step 70 : 1.4718947052955627\n","Step 80 : 1.5404232919216156\n","Step 90 : 1.4204624056816102\n","Step 100 : 1.5532785415649415\n","Step 110 : 1.5206844806671143\n","Step 120 : 1.5877590298652648\n","Step 130 : 1.6004639506340026\n","Step 140 : 1.5943050622940063\n","Step 150 : 1.5294932961463927\n","Step 160 : 1.5288246989250183\n","Step 170 : 1.8098283767700196\n","Step 180 : 1.5419310808181763\n","Step 190 : 1.6439143419265747\n","Step 200 : 1.511615478992462\n","Step 210 : 1.5536516427993774\n","Step 220 : 1.4448819696903228\n","Step 230 : 1.5314794063568116\n","Step 240 : 1.6765119075775146\n","Step 250 : 1.6097695708274842\n","Step 260 : 1.6509467840194703\n","Step 270 : 1.6371580123901368\n","Step 280 : 1.6437106132507324\n","Step 290 : 1.4405755043029784\n","Average Epoch Loss: 1.5820716964864872\n","Epoch 13\n","Step 0 : 1.3513026719912886\n","Step 10 : 1.5193669557571412\n","Step 20 : 1.701940941810608\n","Step 30 : 1.4703899502754212\n","Step 40 : 1.647447657585144\n","Step 50 : 1.5956378340721131\n","Step 60 : 1.695990550518036\n","Step 70 : 1.6053018927574159\n","Step 80 : 1.5674063324928285\n","Step 90 : 1.4828567028045654\n","Step 100 : 1.7210804224014282\n","Step 110 : 1.5734774947166443\n","Step 120 : 1.6095953583717346\n","Step 130 : 1.6738265872001648\n","Step 140 : 1.7802303314208985\n","Step 150 : 1.5621147990226745\n","Step 160 : 1.4357065260410309\n","Step 170 : 1.5963311791419983\n","Step 180 : 1.4942855000495912\n","Step 190 : 1.3178970873355866\n","Step 200 : 1.4918853878974914\n","Step 210 : 1.6656980752944945\n","Step 220 : 1.583921843767166\n","Step 230 : 1.557656168937683\n","Step 240 : 1.5490182518959046\n","Step 250 : 1.393448293209076\n","Step 260 : 1.632590401172638\n","Step 270 : 1.5701551556587219\n","Step 280 : 1.4542673230171204\n","Step 290 : 1.5858848094940186\n","Average Epoch Loss: 1.5735123015012003\n","Epoch 14\n","Step 0 : 1.6110663294792176\n","Step 10 : 2.029900074005127\n","Step 20 : 1.6341030597686768\n","Step 30 : 1.7116285681724548\n","Step 40 : 1.52802734375\n","Step 50 : 1.7661845803260803\n","Step 60 : 1.7582432806491852\n","Step 70 : 1.613861095905304\n","Step 80 : 2.6252963066101076\n","Step 90 : 1.8690042972564698\n","Step 100 : 1.6462525248527526\n","Step 110 : 1.6619152665138244\n","Step 120 : 1.4912832260131836\n","Step 130 : 1.5428643107414246\n","Step 140 : 1.7238707423210144\n","Step 150 : 1.6416891515254974\n","Step 160 : 1.4042570531368255\n","Step 170 : 1.556576907634735\n","Step 180 : 1.5441873788833618\n","Step 190 : 1.673016905784607\n","Step 200 : 1.6944741785526276\n","Step 210 : 1.773996925354004\n","Step 220 : 1.6226737260818482\n","Step 230 : 1.568685245513916\n","Step 240 : 1.7129312992095946\n","Step 250 : 1.592448341846466\n","Step 260 : 1.487179982662201\n","Step 270 : 1.580471432209015\n","Step 280 : 1.5090933680534362\n","Step 290 : 1.5185762643814087\n","Average Epoch Loss: 1.668085778602446\n","Epoch 15\n","Step 0 : 1.4664432287216187\n","Step 10 : 1.4745781302452088\n","Step 20 : 1.5633864402770996\n","Step 30 : 1.5329750299453735\n","Step 40 : 1.5801754236221313\n","Step 50 : 1.449437403678894\n","Step 60 : 1.7623650550842285\n","Step 70 : 1.4559414327144622\n","Step 80 : 1.494618785381317\n","Step 90 : 1.5653082370758056\n","Step 100 : 1.6120825409889221\n","Step 110 : 1.565162682533264\n","Step 120 : 1.6280452370643617\n","Step 130 : 1.5763778924942016\n","Step 140 : 1.8067891120910644\n","Step 150 : 1.702987587451935\n","Step 160 : 1.445455551147461\n","Step 170 : 1.6017412662506103\n","Step 180 : 1.6913232684135437\n","Step 190 : 1.4778803706169128\n","Step 200 : 1.7343668580055236\n","Step 210 : 1.6635727524757384\n","Step 220 : 1.5750520467758178\n","Step 230 : 1.7214659571647644\n","Step 240 : 1.2415863394737243\n","Step 250 : 1.8610112905502318\n","Step 260 : 1.4692039370536805\n","Step 270 : 1.5702249884605408\n","Step 280 : 1.5706088542938232\n","Step 290 : 1.5539504289627075\n","Average Epoch Loss: 1.5884396911470176\n","Epoch 16\n","Step 0 : 1.6446591854095458\n","Step 10 : 1.989194965362549\n","Step 20 : 1.5749765872955321\n","Step 30 : 1.5723470211029054\n","Step 40 : 1.5401247143745422\n","Step 50 : 1.6623470067977906\n","Step 60 : 1.4259334921836853\n","Step 70 : 1.587083888053894\n","Step 80 : 1.5746344923973083\n","Step 90 : 1.4904719471931458\n","Step 100 : 1.565251874923706\n","Step 110 : 1.4016470789909363\n","Step 120 : 1.576850128173828\n","Step 130 : 1.7247864246368407\n","Step 140 : 1.699311649799347\n","Step 150 : 1.4765097498893738\n","Step 160 : 1.8263611197471619\n","Step 170 : 1.440930712223053\n","Step 180 : 1.4371585965156555\n","Step 190 : 1.4646069645881652\n","Step 200 : 1.5540979743003844\n","Step 210 : 1.574343776702881\n","Step 220 : 1.5318508267402648\n","Step 230 : 1.5840554237365723\n","Step 240 : 1.6140058994293214\n","Step 250 : 1.6156505227088929\n","Step 260 : 1.5270668864250183\n","Step 270 : 1.6149725794792176\n","Step 280 : 1.6106476783752441\n","Step 290 : 1.4858495235443114\n","Average Epoch Loss: 1.5717795046316052\n","Epoch 17\n","Step 0 : 1.3070891100913287\n","Step 10 : 1.6694341540336608\n","Step 20 : 1.6016186952590943\n","Step 30 : 1.6684072136878967\n","Step 40 : 1.6118425607681275\n","Step 50 : 1.5083310842514037\n","Step 60 : 1.387621569633484\n","Step 70 : 1.6587429881095885\n","Step 80 : 1.7206423997879028\n","Step 90 : 1.5607022285461425\n","Step 100 : 1.682567286491394\n","Step 110 : 1.7296265065670013\n","Step 120 : 1.548151171207428\n","Step 130 : 1.4748667359352112\n","Step 140 : 1.4656371235847474\n","Step 150 : 1.5185107350349427\n","Step 160 : 1.4266735434532165\n","Step 170 : 1.4756725788116456\n","Step 180 : 1.4608765900135041\n","Step 190 : 1.4904906451702118\n","Step 200 : 1.5190446376800537\n","Step 210 : 1.587452471256256\n","Step 220 : 1.6586888909339905\n","Step 230 : 1.5296169638633728\n","Step 240 : 1.5087408423423767\n","Step 250 : 1.5758469343185424\n","Step 260 : 1.6265758752822876\n","Step 270 : 1.5729270219802856\n","Step 280 : 1.6764259219169617\n","Step 290 : 1.7051709175109864\n","Average Epoch Loss: 1.5729233714867923\n","Epoch 18\n","Step 0 : 1.6174106001853943\n","Step 10 : 1.7072444200515746\n","Step 20 : 1.5410966873168945\n","Step 30 : 1.4900797367095948\n","Step 40 : 1.5708524227142333\n","Step 50 : 1.6884498119354248\n","Step 60 : 1.5228485226631165\n","Step 70 : 1.5772797226905824\n","Step 80 : 1.7064518094062806\n","Step 90 : 1.3511943817138672\n","Step 100 : 1.5765440344810486\n","Step 110 : 1.4222636342048645\n","Step 120 : 1.3992845058441161\n","Step 130 : 1.5577992796897888\n","Step 140 : 1.8395412683486938\n","Step 150 : 1.678876781463623\n","Step 160 : 1.551486474275589\n","Step 170 : 1.3681900560855866\n","Step 180 : 1.6098644852638244\n","Step 190 : 1.6897025465965272\n","Step 200 : 1.5616437196731567\n","Step 210 : 1.5764028191566468\n","Step 220 : 1.4780429124832153\n","Step 230 : 1.3934617936611176\n","Step 240 : 1.6050016045570374\n","Step 250 : 1.5366955161094666\n","Step 260 : 1.352200174331665\n","Step 270 : 1.7575416803359984\n","Step 280 : 1.581957221031189\n","Step 290 : 1.6886209845542908\n","Average Epoch Loss: 1.5605013097043743\n","Epoch 19\n","Step 0 : 1.3960083051584662\n","Step 10 : 1.5891530394554139\n","Step 20 : 1.4641343235969544\n","Step 30 : 1.505738127231598\n","Step 40 : 1.770171332359314\n","Step 50 : 1.5698045253753663\n","Step 60 : 1.563934600353241\n","Step 70 : 1.4116131663322449\n","Step 80 : 1.7096125841140748\n","Step 90 : 1.6059900522232056\n","Step 100 : 1.558618974685669\n","Step 110 : 1.5699745535850524\n","Step 120 : 1.761509907245636\n","Step 130 : 1.7705307602882385\n","Step 140 : 1.6366098642349243\n","Step 150 : 1.4954817295074463\n","Step 160 : 1.3996288776397705\n","Step 170 : 1.542446458339691\n","Step 180 : 1.435717523097992\n","Step 190 : 1.4576398015022278\n","Step 200 : 1.543528687953949\n","Step 210 : 1.5789530396461486\n","Step 220 : 1.54669508934021\n","Step 230 : 1.452308475971222\n","Step 240 : 1.4930216431617738\n","Step 250 : 1.4823060989379884\n","Step 260 : 1.6397753834724427\n","Step 270 : 1.4377641558647156\n","Step 280 : 1.6285335183143617\n","Step 290 : 1.5883503556251526\n","Average Epoch Loss: 1.552434215039918\n","Epoch 20\n","Step 0 : 1.3761426270008088\n","Step 10 : 1.4793243885040284\n","Step 20 : 1.5778740286827087\n","Step 30 : 1.4313708305358888\n","Step 40 : 1.5734127461910248\n","Step 50 : 1.6623722672462464\n","Step 60 : 1.4800383925437928\n","Step 70 : 1.5626100301742554\n","Step 80 : 1.5257899165153503\n","Step 90 : 1.4678353309631347\n","Step 100 : 1.5294471263885498\n","Step 110 : 1.6511679410934448\n","Step 120 : 1.5036559462547303\n","Step 130 : 1.5841994643211366\n","Step 140 : 1.6755050659179687\n","Step 150 : 1.6875625729560852\n","Step 160 : 1.5603934526443481\n","Step 170 : 1.5806886434555054\n","Step 180 : 1.6824631214141845\n","Step 190 : 1.4996302247047424\n","Step 200 : 1.5015559196472168\n","Step 210 : 1.4356574892997742\n","Step 220 : 1.5068900346755982\n","Step 230 : 1.7364009976387025\n","Step 240 : 1.7566551566123962\n","Step 250 : 1.6989286541938782\n","Step 260 : 1.561800765991211\n","Step 270 : 1.5847935676574707\n","Step 280 : 1.584889578819275\n","Step 290 : 1.6252023577690125\n","Average Epoch Loss: 1.5701776524184128\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iIw-6S2_d6L6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":281},"outputId":"38d4a961-a53c-4bce-acb8-9d55c2923e53","executionInfo":{"status":"ok","timestamp":1585891082477,"user_tz":360,"elapsed":715,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}}},"source":["a = [23.63948760032654,3.7310128688812254,3.5388919830322267,3.538793110847473,2.7908554077148438,2.5297589182853697,2.3980841398239137,2.4206475734710695,2.2256847381591798,1.8266412615776062,1.8226126432418823,1.5747804522514344,1.7296615242958069,1.8519200921058654,1.6441534876823425,1.5568920850753785,1.9605435609817505,1.5984519720077515,1.6057345032691956,1.6863449811935425,1.6038021206855775,1.6635065078735352,1.8346874356269836,1.7405985355377198,1.9457938551902771,1.9671810150146485,1.8160136222839356,1.6802731275558471,1.6670244097709657,1.6649189829826354,1.9788997769355774,2.287485146522522,1.7207539796829223,1.652290689945221,1.8845996379852294,2.249750077724457,1.9487884759902954,1.6277625560760498,1.6056967735290528,1.6372004985809325,1.5774993181228638,1.5866549611091614,1.4299665451049806,1.5778466820716859,1.5760279297828674,1.5161873698234558,1.4124821543693542,1.6462961554527282,1.7293179988861085,1.509164822101593,1.4679332613945006,1.5912979245185852,1.567425000667572,1.5821929752826691,1.5926974654197692,1.3974286437034606,1.5099110722541809,1.5787468671798706,1.6186208963394164,1.3620702657848596,1.2463204741477967,1.4407684445381164,1.5221697807312011,1.4963760495185852,1.509787654876709,1.8561355948448182,1.7647417783737183,1.5217341184616089,1.6890993595123291,1.4868379592895509,1.3642069697380066,1.5997826218605042,1.4386527061462402,1.4967490315437317,1.7840389609336853,1.527709698677063,1.5122908353805542,1.460020363330841,1.5327269554138183,1.7089971363544465,1.529446816444397,1.6548919558525086,1.5933759450912475,1.6371606111526489,1.4974799513816834,1.6630700469017028,1.6147354543209076,1.375999289751053,1.5929417848587035,1.504815047979355,1.7267858862876893,1.5673058032989502,1.626545000076294,1.3785046219825745,1.3876992523670197,1.4210498332977295,1.6211077332496644,1.3902597665786742,1.611822748184204,1.5364004373550415,1.6773049354553222,1.5308062791824342,1.3861291885375977,1.5128313899040222,1.5571601510047912,1.720938467979431,1.5696717262268067,1.5522677779197693,1.6919808864593506,1.540165913105011,1.6423703789711,1.4338365137577056,1.6110373616218567,1.6666245341300965,1.595628809928894,1.4882213652133942,1.6022398114204406,1.626602792739868,1.443321907520294,1.374821405520197,1.6518424153327942,1.349837863445282,1.528708815574646,1.591989815235138,1.5234298825263977,1.5604309558868408,1.686996829509735,1.5526540875434875,1.7323722839355469,1.456289291381836,1.4458529949188232,1.6670520186424256,1.4820118427276612,1.7638343095779419,1.5394953846931458,1.4708619594573975,1.7182538986206055,1.5155243098735809,1.3711166501045227,1.5004495322704314,1.6547746062278748,1.4915873885154725,1.4776412010192872,1.4976331114768981,1.5526248574256898,1.5113723039627076,1.5231546759605408,1.4457776546478271,1.6298669815063476,1.7355843782424927,2.919375705718994,2.3694878697395323,1.8352251648902893,2.6956708312034605,2.655014765262604,2.379216170310974,2.915577471256256,3.512195897102356,2.7202182054519652,2.321783256530762,2.4433849096298217,2.7967835903167724,2.419164609909058,2.52241907119751,2.946675157546997,2.7046121835708616,2.6445175647735595,3.0925604581832884,3.7130346059799195,2.766263222694397,2.6533337354660036,2.5109963536262514,2.667663311958313,3.6165145874023437,3.7686266183853148,2.953689897060394,2.887112045288086,2.9175302267074583,2.1590128540992737,2.102255177497864,1.8610278487205505,2.0883387327194214,2.0658130764961244,3.0008469104766844,2.824829030036926,3.0706108927726747,2.700793731212616,2.04626704454422,2.282307356595993,3.0106335997581484,2.551390993595123,2.2870928049087524,1.6550434589385987,1.7686383128166199,1.7955866813659669,1.9724652290344238,1.8153830647468567,1.6705814003944397,1.8769320964813232,2.32726628780365,1.8890939354896545,1.783400058746338,2.5101970434188843,4.130855226516724,2.3215047478675843,2.2333560824394225,1.8047037720680237,1.7877434372901917,1.819242489337921,1.6798553049564362,1.8983316898345948,1.6372491836547851,1.5657285928726197,1.7824323177337646,1.7220169067382813,1.6010552763938903,1.769887101650238,1.5856457829475403,1.7303412675857544,1.838882577419281,1.7271790385246277,1.7522998929023743,1.6796557545661925,1.4936877369880677,1.5807045698165894,1.8380472302436828,1.6671354293823242,1.3688917279243469,1.5277010440826415,1.590848183631897,1.7210683941841125,1.5882745146751405,1.4373624175786972,1.6989512205123902,1.60247243642807,1.5623340129852294,1.5208447217941283,1.5443803071975708,1.5816893100738525,1.5711776733398437,1.4454649567604065,1.789099383354187,1.7764584302902222,1.4897143483161925,1.4854795157909393,1.5322001338005067,1.7311649918556213,1.5093348801136017,2.106648087501526,1.8912765026092528,1.703181529045105,1.7695202350616455,1.7854139447212218,1.6352163910865785,1.6492251396179198,1.798626220226288,1.4799168705940247,1.5728116393089295,1.6875527322292327,1.708535385131836,1.6227306067943572,1.7078002452850343,1.4410066939890385,1.910949945449829,1.6095294594764709,1.5855515718460083,1.6887520551681519,1.6368744015693664,1.520090651512146,1.635040295124054,1.5030403971672057,1.448222541809082,1.5849572777748109,1.555473017692566,1.4871482610702516,1.6298844695091248,1.7599026679992675,1.6785809516906738,1.6104175090789794,1.442113184928894,1.741139256954193,1.5580462217330933,1.4894386649131774,1.6667961120605468,1.4518734693527222,1.4925562739372253,1.5075868487358093,1.5116301774978638,1.6438276410102843,1.8112398862838746,1.4602665305137634,1.5610406875610352,1.9577155232429504,2.332210457324982,1.8155151724815368,1.6752721548080445,1.5336487770080567,1.4800007820129395,1.3039790272712708,1.5628435492515564,1.6641686677932739,1.559764325618744,1.7420625925064086,1.5790706992149353,1.6785444617271423,1.7483733654022218,1.653809404373169,1.6358391046524048,1.6674647092819215,1.588201093673706,1.4968189954757691,1.457471239566803,1.7276102304458618,1.518505072593689,1.5403757691383362,1.4711898684501648,1.5182250380516051,1.772458589076996,1.8027240872383117,1.4997762322425843,1.629825234413147,1.5071747720241546,1.5109777241945266,1.6794643640518188,1.769541597366333,1.6704235315322875,1.5313626885414124,1.867364728450775,1.540811550617218,1.4718947052955627,1.5404232919216156,1.4204624056816102,1.5532785415649415,1.5206844806671143,1.5877590298652648,1.6004639506340026,1.5943050622940063,1.5294932961463927,1.5288246989250183,1.8098283767700196,1.5419310808181763,1.6439143419265747,1.511615478992462,1.5536516427993774,1.4448819696903228,1.5314794063568116,1.6765119075775146,1.6097695708274842,1.6509467840194703,1.6371580123901368,1.6437106132507324,1.4405755043029784,1.3513026719912886,1.5193669557571412,1.701940941810608,1.4703899502754212,1.647447657585144,1.5956378340721131,1.695990550518036,1.6053018927574159,1.5674063324928285,1.4828567028045654,1.7210804224014282,1.5734774947166443,1.6095953583717346,1.6738265872001648,1.7802303314208985,1.5621147990226745,1.4357065260410309,1.5963311791419983,1.4942855000495912,1.3178970873355866,1.4918853878974914,1.6656980752944945,1.583921843767166,1.557656168937683,1.5490182518959046,1.393448293209076,1.632590401172638,1.5701551556587219,1.4542673230171204,1.5858848094940186,1.6110663294792176,2.029900074005127,1.6341030597686768,1.7116285681724548,1.52802734375,1.7661845803260803,1.7582432806491852,1.613861095905304,2.6252963066101076,1.8690042972564698,1.6462525248527526,1.6619152665138244,1.4912832260131836,1.5428643107414246,1.7238707423210144,1.6416891515254974,1.4042570531368255,1.556576907634735,1.5441873788833618,1.673016905784607,1.6944741785526276,1.773996925354004,1.6226737260818482,1.568685245513916,1.7129312992095946,1.592448341846466,1.487179982662201,1.580471432209015,1.5090933680534362,1.5185762643814087,1.4664432287216187,1.4745781302452088,1.5633864402770996,1.5329750299453735,1.5801754236221313,1.449437403678894,1.7623650550842285,1.4559414327144622,1.494618785381317,1.5653082370758056,1.6120825409889221,1.565162682533264,1.6280452370643617,1.5763778924942016,1.8067891120910644,1.702987587451935,1.445455551147461,1.6017412662506103,1.6913232684135437,1.4778803706169128,1.7343668580055236,1.6635727524757384,1.5750520467758178,1.7214659571647644,1.2415863394737243,1.8610112905502318,1.4692039370536805,1.5702249884605408,1.5706088542938232,1.5539504289627075,1.6446591854095458,1.989194965362549,1.5749765872955321,1.5723470211029054,1.5401247143745422,1.6623470067977906,1.4259334921836853,1.587083888053894,1.5746344923973083,1.4904719471931458,1.565251874923706,1.4016470789909363,1.576850128173828,1.7247864246368407,1.699311649799347,1.4765097498893738,1.8263611197471619,1.440930712223053,1.4371585965156555,1.4646069645881652,1.5540979743003844,1.574343776702881,1.5318508267402648,1.5840554237365723,1.6140058994293214,1.6156505227088929,1.5270668864250183,1.6149725794792176,1.6106476783752441,1.4858495235443114,1.3070891100913287,1.6694341540336608,1.6016186952590943,1.6684072136878967,1.6118425607681275,1.5083310842514037,1.387621569633484,1.6587429881095885,1.7206423997879028,1.5607022285461425,1.682567286491394,1.7296265065670013,1.548151171207428,1.4748667359352112,1.4656371235847474,1.5185107350349427,1.4266735434532165,1.4756725788116456,1.4608765900135041,1.4904906451702118,1.5190446376800537,1.587452471256256,1.6586888909339905,1.5296169638633728,1.5087408423423767,1.5758469343185424,1.6265758752822876,1.5729270219802856,1.6764259219169617,1.7051709175109864,1.6174106001853943,1.7072444200515746,1.5410966873168945,1.4900797367095948,1.5708524227142333,1.6884498119354248,1.5228485226631165,1.5772797226905824,1.7064518094062806,1.3511943817138672,1.5765440344810486,1.4222636342048645,1.3992845058441161,1.5577992796897888,1.8395412683486938,1.678876781463623,1.551486474275589,1.3681900560855866,1.6098644852638244,1.6897025465965272,1.5616437196731567,1.5764028191566468,1.4780429124832153,1.3934617936611176,1.6050016045570374,1.5366955161094666,1.352200174331665,1.7575416803359984,1.581957221031189,1.6886209845542908,1.3960083051584662,1.5891530394554139,1.4641343235969544,1.505738127231598,1.770171332359314,1.5698045253753663,1.563934600353241,1.4116131663322449,1.7096125841140748,1.6059900522232056,1.558618974685669,1.5699745535850524,1.761509907245636,1.7705307602882385,1.6366098642349243,1.4954817295074463,1.3996288776397705,1.542446458339691,1.435717523097992,1.4576398015022278,1.543528687953949,1.5789530396461486,1.54669508934021,1.452308475971222,1.4930216431617738,1.4823060989379884,1.6397753834724427,1.4377641558647156,1.6285335183143617,1.5883503556251526,1.3761426270008088,1.4793243885040284,1.5778740286827087,1.4313708305358888,1.5734127461910248,1.6623722672462464,1.4800383925437928,1.5626100301742554,1.5257899165153503,1.4678353309631347,1.5294471263885498,1.6511679410934448,1.5036559462547303,1.5841994643211366,1.6755050659179687,1.6875625729560852,1.5603934526443481,1.5806886434555054,1.6824631214141845,1.4996302247047424,1.5015559196472168,1.4356574892997742,1.5068900346755982,1.7364009976387025,1.7566551566123962,1.6989286541938782,1.561800765991211,1.5847935676574707,1.584889578819275,1.6252023577690125]\n","plt.plot(a)\n","plt.title(\"Regression Loss\")\n","plt.show()"],"execution_count":77,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXyU5b338c8vC4Q1bAHCrqgsLoCi\n4g4WFa1LT+v61GpbW4/n2MfltHXp6WPbU3vs6latVutSq3Xf0arsoiAYkE22hCUQCCQkQFayXs8f\nc8+STAIhBCYXft+vV16Zueee+76uyeQ71/zuzZxziIiIf5IS3QAREWkdBbiIiKcU4CIinlKAi4h4\nSgEuIuIpBbiIiKcU4CKAmZ1lZmsS3Q6R/WHaD1zaipltBPoBdUAZ8AHwI+dcWSLblUhmNgzYAKQ6\n52oT2xo53GgELm3tEudcV2AsMA64u61XYGYpbb1MER8pwOWgcM5tAz4kFOQAmNkEM5tnZrvMbKmZ\nTYx57Agz+9jMSs1supk9ambPB48NMzNnZjeY2SZgZjD9+2a2ysx2mtmHZjY0mG5m9oCZFZhZiZkt\nN7PjgscuMrOVwXq2mNlPgukTzSwvpj2jzGx20NYvzezSmMeeDdr3XrCcBWY2fH9fIzMbYGbvmFmx\nmeWY2Q9jHjvFzLKC9m83s/uD6Wlm9ryZFQVt+9zM+u3vuuXwoACXg8LMBgEXAjnB/YHAe8C9QC/g\nJ8DrZpYRPOWfwEKgN/BL4DtNLPYcYBRwgZldBvwM+CaQAcwFXgzmOx84GzgGSAeuBIqCx54C/t05\n1w04juDDoFHbU4F3gY+AvsD/BV4wsxExs10N/AroGfTxNy16YRp6CcgDBgCXA/9rZucGjz0EPOSc\n6w4MB14Jpl8f9GkwodfqJqCyFeuWw4ACXNraW2ZWCmwGCoBfBNOvBd53zr3vnKt3zk0DsoCLzGwI\ncDJwj3Ou2jn3CfBOE8v+pXOu3DlXSSi47nPOrQpqy/8LjA1G4TVAN2Akoe08q5xz+cEyaoDRZtbd\nObfTObe4ifVMALoCvw3aMxOYClwTM8+bzrmFwbpfIOabRkuY2WDgDOBO59we59wS4G/AdTHtPMrM\n+jjnypxzn8VM7w0c5Zyrc84tcs6V7M+65fChAJe29o1gdDuRUID2CaYPBa4IvvbvMrNdwJlAJqER\naLFzriJmOZubWHbstKHAQzHLKgYMGBgE7iPAo0CBmT1hZt2D530LuAjINbM5ZnZaE+sZAGx2ztXH\nTMsFBsbc3xZzu4JQ4O+PcJ9Lm1nHDYS+QawOyiQXB9P/Qag09ZKZbTWz3wffGOQrSAEuB4Vzbg7w\nLPDHYNJm4B/OuR4xP12cc78F8oFeZtY5ZhGDm1pszO3NhEohscvr5JybF6z/YefcScBoQkH402D6\n5865ywiVRt4iWpqItRUYbGax/x9DgC379SLs3VZCfe7W1Dqcc9nOuWuCdv4OeM3Mujjnapxzv3LO\njQZOBy4mOmqXrxgFuBxMDwLnmdkY4HngEjO7wMySg41xE81skHMul1A55Zdm1iEYFV+yj2U/Dtxt\nZscCmFm6mV0R3D7ZzE4NRqblwB6gPlj2t80s3TlXA5QA9U0sewGhUfUdZpYabGy9hFDNurU6Bn1O\nM7M0QkE9D7gvmHYCoVF3eMPttWaWEXwL2BUso97MJpnZ8WaWHLS/ppk+yFeAAlwOGudcIfAcodr2\nZiC84bGQ0Aj6p0Tfg98GTiO0sfFe4GWgai/LfpPQyPQlMysBVhDaaArQHXgS2EmoLFEE/CF47DvA\nxuA5NwXrbbzsakKBfSGwA/gLcJ1zbvV+vwhRZYQ2NoZ/ziVUUx9GaDT+JvAL59z0YP4pwJdmVkZo\ng+bVQe2/P/AaofBeBcwhVFaRryAdyCPtkpm9DKx2zv1inzOLfEVpBC7tQlD2GG5mSWY2hdBo/a1E\nt0ukPdMRbdJe9AfeILSLXB7wH865LxLbJJH2TSUUERFPqYQiIuKpQ1pC6dOnjxs2bNihXKWIiPcW\nLVq0wzmX0Xj6IQ3wYcOGkZWVdShXKSLiPTPLbWq6SigiIp5SgIuIeEoBLiLiKQW4iIinFOAiIp5S\ngIuIeEoBLiLiKS8CfMaq7Tw2e12imyEi0q54EeCz1hTw5Nz1iW6GiEi74kWAG5boJoiItDteBDiA\nzpooItKQFwFu1vBqtiIi4kuAAxqAi4g05EeAm2rgIiKNeRHgoBq4iEhj/gR4ohsgItLOeBHgZijB\nRUQa8SPAtR+4iEgcLwIcNAAXEWnMiwA300ZMEZHG/AhwNAIXEWnMjwBXCVxEJI4XAQ46ElNEpDEv\nAtzMcCqiiIg04EeAoxG4iEhjXgS4dgMXEYnnR4CjvVBERBrzIsANnRBcRKQxPwLc0EZMEZFG/Ajw\nRDdARKQd8iLAQXuhiIg05kWA65qYIiLx/AhwTCezEhFpxI8AVxFcRCSOFwEOKqGIiDTmRYDrUHoR\nkXheBLhqKCIi8fYZ4GY22MxmmdlKM/vSzG4Npvcys2lmlh387nmwGhmOb23IFBGJaskIvBb4sXNu\nNDABuNnMRgN3ATOcc0cDM4L7IiJyiOwzwJ1z+c65xcHtUmAVMBC4DPh7MNvfgW8crEaGKygagIuI\nRO1XDdzMhgHjgAVAP+dcfvDQNqBfM8+50cyyzCyrsLCwVY20oIii/BYRiWpxgJtZV+B14DbnXEns\nYy5UnG4yX51zTzjnxjvnxmdkZLSqkdERuCJcRCSsRQFuZqmEwvsF59wbweTtZpYZPJ4JFBycJupk\nViIiTWnJXigGPAWscs7dH/PQO8D1we3rgbfbvnkNafwtIhKV0oJ5zgC+Ayw3syXBtJ8BvwVeMbMb\ngFzgyoPTRG3EFBFpyj4D3Dn3Cc1XMb7Wts1pmll4I6YSXEQkzI8jMUVEJI5XAa4SiohIlBcBrlOh\niIjE8yPAwwfyaAQuIhLhR4BrBC4iEseLAA/TXigiIlFeBHj0dLIJbYaISLviR4CHD+RJbDNERNoV\nPwJcZ0MREYnjRYCH6WyEIiJRXgS4SigiIvG8CPAwDcBFRKK8CHDTjuAiInG8CPAIjcBFRCK8CPDI\nfuBKcBGRCD8CXBd0EBGJ40eAJ7oBIiLtkBcBHqYBuIhIlBcBHrmkmmooIiIRngR46LfiW0Qkyo8A\nT3QDRETaIS8CPEwVFBGRKD8CPFwDVxFFRCTCiwCPlFCU3yIiEX4EuIrgIiJxvAjwMA3ARUSivAjw\n8BV5tBFTRCTKjwCP7AeuBBcRCfMjwBPdABGRdsiLAA9TCUVEJMqLANeh9CIi8fwIcHQyKxGRxrwI\ncBXBRUTi+RHgAQ3ARUSivAhwDcBFROLtM8DN7GkzKzCzFTHTfmlmW8xsSfBz0cFsZPSCDgdzLSIi\nfmnJCPxZYEoT0x9wzo0Nft5v22Y1pBG4iEi8fQa4c+5joPgQtGWfdCSmiEjUgdTAf2Rmy4ISS8/m\nZjKzG80sy8yyCgsLW7WiyH7gym8RkYjWBvhjwHBgLJAP/Km5GZ1zTzjnxjvnxmdkZLRqZTqQR0Qk\nXqsC3Dm33TlX55yrB54ETmnbZjVkqoKLiMRpVYCbWWbM3X8DVjQ3b1vSkZgiIlEp+5rBzF4EJgJ9\nzCwP+AUw0czGEqpqbAT+/SC2USUUEZEm7DPAnXPXNDH5qYPQln3SAFxEJMqPIzF1UUwRkTheBHiU\nhuAiImFeBHh4/K0SiohIlB8Bro2YIiJx/AhwXZVeRCSOFwEuIiLxvAjwaAlFQ3ARkTA/Ajz4rRKK\niEiUHwGusxGKiMTxIsBFRCSeJwEe7IWiGriISIQXAa4SiohIPD8CPNENEBFph7wIcBERiedFgIfP\nRqgSiohIlB8BHvzWRkwRkSg/AlwbMUVE4ngV4CIiEuVFgIdpAC4iEuVFgEdPJ6sIFxEJ8yLA0QUd\nRETieBHgKoGLiMTzIsDDVEEREYnyIsDNonuCi4hIiB8BHvzWCFxEJMqPAFcRXEQkjhcBHqYBuIhI\nlBcBHt0PPMENERFpR/wI8Mi5UJTgIiJhfgR4ohsgItIOeRHgYRp/i4hE+RHgOp2siEgcLwLcdFV6\nEZE4fgS4iuAiInG8CPAIDcBFRCL2GeBm9rSZFZjZiphpvcxsmpllB797HsxG6kwoIiLxWjICfxaY\n0mjaXcAM59zRwIzg/kGjq9KLiMTbZ4A75z4GihtNvgz4e3D778A32rhdDagGLiISr7U18H7Oufzg\n9jagXxu1Z6+0F4qISNQBb8R0oePbm01WM7vRzLLMLKuwsLBV69DpZEVE4rU2wLebWSZA8LuguRmd\nc08458Y758ZnZGS0amWma2KKiMRpbYC/A1wf3L4eeLttmtMcFcFFRBpryW6ELwLzgRFmlmdmNwC/\nBc4zs2xgcnD/oNPZCEVEolL2NYNz7ppmHvpaG7elWSqhiIjE8+JIzEgBRQkuIhLhR4BrR3ARkThe\nBHiY9gMXEYnyIsC1H7iISDw/AlwXdBARieNHgGs/cBGROF4EeJgG4CIiUV4EeLSEoggXEQnzIsDD\nFN8iIlFeBLh2AxcRiedFgIepgiIiEuVFgJuuiikiEsePANd+4CIicbwKcBERifIiwMM0ABcRifIi\nwMM1cJVQRESi/AjwyAUdlOAiImF+BHiiGyAi0g55EeBhKqGIiER5EeC6JqaISDwvApzIRkxFuIhI\nmBcBrv3ARUTieRHgIiISz4sA1zUxRUTi+RHgQQ1F+4GLiET5EeDBb43ARUSivAhwERGJ50WA63Sy\nIiLx/Ajw8H7gCW6HiEh74keA66r0IiJxvAhwERGJ51WAa/wtIhLlRYCbrmksIhLHkwDXgTwiIo15\nEeAiIhLPiwDXkZgiIvFSDuTJZrYRKAXqgFrn3Pi2aFT8ekK/ld8iIlEHFOCBSc65HW2wnGbpqvQi\nIvG8KKGIiEi8Aw1wB3xkZovM7MamZjCzG80sy8yyCgsLW7WSaAlFQ3ARkbADDfAznXMnAhcCN5vZ\n2Y1ncM494Zwb75wbn5GR0aqVaCOmiEi8Awpw59yW4HcB8CZwSls0Ko42YoqIxGl1gJtZFzPrFr4N\nnA+saKuGNVgXuqqxiEhjB7IXSj/gzeAoyRTgn865D9qkVc1RDUVEJKLVAe6cWw+MacO2NEv7gYuI\nxPNiN0JtxGz/Vm8robK6LtHNEPlK8SPATTXw9qy8qpYpD87lv15ZkuimiHyleBHgYboiT/tUWRMa\neS/cUJzgloh8tXgR4Dod+KFRWV1Hff3+v8rVtfUHoTUisi9+BHiQ4K3IFtkL5xyPzsphc3EFFdW1\njLrnA+6ftna/lxMegavSJXJoeRHg3dJS6dk5lXk5B/WcWV85m4or+MOHa/jRPxfz4ZfbACK/98ee\nmvDGSyW4yKHkRYAnJxmTRvRlxuoCXsnanOjmeOmBaWv53QerG0zbXVkDQEV1HdnbywA4ul/X/V72\nnppQCUUjcJFDy4sAB7jropEArC8sT3BL/PTQjGwem72uwbQdZVUAdExNomRPKMzDob4/qsIllANs\no4jsH28CvG+3NPp178jO8upEN+WwUVgaBHhKMqV7agHYVbH/Aa4auEhieBPgAD07d6C4QgG+v2rr\nonuJxO6KWVASDvAkSoKRd2sCPFJC0Rhc5JDyLsA1At9/O8qir1llTfRoyYJgBF5VW09JZAQenbem\nrp4nP17P8b/8kK27Knl/eX6Tuxnu0QhcJCG8CvBeXTQCb6ldFdXc/9Eaauvq2VayJzJ9Z0UNFdW1\nXPXX+by/PB+AksqayAi8vLqO+/61CuccLy3cxG/eX0Xpnlp+/MpS/vOFxbywIDduXXtqVQMXSQSv\nArxnl1SNwFto9ppCHp6Zw/Itu9keE+Bz1hTyyMwcFmwopih4LUv21FC6p5ZTjugFwF/nrGd7SRXl\nMec2WbWtBIB1TWxEDp8DRac8EDm02uKixodMZnondlbUsLuyhvROqYluTrtWVhUqiTz96UbWbiuN\nTP/Zm8vj5i0qq6a23nHJmEwuP2kQd7y2jJ0V1dTFlEvCtfGK6tq451fpSEyRhPBqBD52cA8Almze\nleCWtH/hoH136VbWbC9tdr6bzhlObRDU/bqnMbhnZwB2lldTsqeGDilJnDKsV2T+8N4qscI18Hqd\nqyZhrnx8Phc9NDfRzZBDzKsR+JjBPUhOMp7/LJdTj+hFWmpyopvUbpVV7fvUrh/edjZH9OnCtJXb\nWFdYzpTj+lMePG9nRQ0llbV0T0ulT7cOkeds2NF8CUXnREmchRt1IrGvIq9G4F07pjDluP5MW7md\nB6dnJ7o57VpFVfxIOda1E4Ywon83OqQk8cIPJvD0d8czqGdnenYOlaaKK0Ij8O6dUujTtWPkeWu3\nl0bKMwBzswvJD2rsPpZScgrKWJanb3TiJ68CHOAXl4wG4IMV+S1+zq0vfcG4//mI+euKWnWyJh+V\nx9Sqxw7uwTPfO5nJo/rRKTWZj24/m/+59LjI4/3T0zh3ZD8AenQOjbZ3lldTUlkTGoHHBHi9g+V5\nu4HQaPw7Ty3kvWWhv0VZVS1LPStvTb5/Dpc+8mmim9FmdMrlrxbvArxvtzR+/vVRbCyqiBwKvi9v\nL9nKzooarnnyMx6ekd1gBHm4Ko8poXzrxIFMGtGXx689kS/uOY9j+nUjKanpPUY6pCTRrWMKn28s\nZm72Drp0TKZHMCof0a8bANtKKgHI31UZed7YwT3okJLEe8tb/sHaUp9vLG6ydNOWDpfgK/0KvLcl\nyrsABxjeN3TCpY3BP7VzLm73woLSPRSXVzc4CjFsdX7JwW9kgpXH/CN3D/bYSUlOatF2g6tOHszc\n7NCZH3t16Ri5lN15o0Oj9MLSKlZvK2FTcUXkOUN7d2ZIr87kFrV90F7x+Hwm/XF2my83do+akkq/\ngm9zcQU3v7CYiuraBu/xQ7GbbemeGj5eW3jQ1yP75mWAH9G7CwDrd5RTVFbF85/lMu7X08gO9rbI\n3l7KKb+ZwVV/nd/kyO3Lra0L8PWFZa264EEiHMi3jOtOGxa5/bOLRnLVyYP574tGccvXjqZjShLT\nVxYw5cG5DXZJ7Nm5A0N7dSa3qILNxRU8MjO7wWtVXVsfCZr6etfkB2tTYpdR3kSfqmrruPO1ZWyO\n+TBpqY07os/ZurtyL3Pu28INxQ12uzzY7vvXKt5bns+s1YUN/tbFBynAy6tqIwOm219ewnVPL6Qg\n5viC9qC2rv6Q/g3aAy8DfFDPTiQnGXe8toyT7p3O/3v7SyAUzMvzdnPeAx8DkF1QxtOfbox7/som\nAryyum6vobJt9x7O/dMcbn05et3HDTvKefLj9Xv9+r2npvm9QZxz+/XVvaq2LjJqnL5y+15DuiLm\nIJzuafu3z/zgXp0AOO3I3mSmdyItNZkfnn0kHVKS6NWlQ2SPh9j/laraOob07szqbaWc9ftZ/PGj\ntfxp2hpmrt7OSws38c3HPuWcP8xmU1EF3//750y+f05oGU38w63dXsrdbyxnzbZStsSUaf7tL5/y\n0sJN3PrSF5GR5uLcXbyctZk7X1/WxGtQy7ycHdTXu7iAf3D62gbX8Iz9NrGpqILSPTWsLyxr0L+6\nekd5VS13vLaUV4PTGi/P281fZudw5V/n88TH6xuswznHj/65mDn7MVp1zlFX73h8zjry9/KhUlsX\net12lFXx2JzoWSY3FVfgnKMmeC8753h9UV7kxGWN1dTV8+GX2yLvw53l1Vz39MLIYKiwtArnHP/x\nwmIm/nE2tXX1kUvn5TbxoVldW8/tLy/he88sbHGf6+tD7a1p4Yd6cyb+cTbfbWa9n2TviCx/c3EF\n01Zujzy2u6KGv81dH+lzW1mwvoh1Me+hg8Gr3QjDUpKT+M6EoTw7b2OD6Xe9sSxyYqW01CQ6piTz\n4sJNDeYZ0qszM1ZvZ3nebo4flA6E/oAn/2Y63zppIPd984TIvNW19dQ7R1pqMivzQxvu3l26lfRO\nKdz7jeO58bkssgvKuODY/tzzzgpSkowrxw+mvLqWPl07kt4plUsf+ZQ/XjGGy08aRFFZFZ06JHP/\nR2u55tQhTF+5nT/PzGHiiAyGZ3Tl7GMyWLihmOtPH8qO0uh+2Mf068aLCzdx9xvL6dwhmUvHDOCl\nz0MBkt4plQ9vO5svNu3kwuMzgdCH0ZZdlVw6ZgCXjBnAxBEZ+/X6mhlL7zmfjqnxn+/5u0OjriQL\nBfi4IT3okJzEN8YOpGNqMm8s3hI5Je2js9bFPf/SRz+JHBT0ac4OvvfM50wckUFqchIPXzOO5CTj\nlc838+LCTVTV1vHG4i2R567dXsZdb4RG/avyS1i7PfrPMW9dEXe/sYzLTxrMsQO6k5aazG//tZrn\n5udydN+uZBeUcdvkoznjqD6MyuzeYC+mDilJvLt0K+eN6sdDM7J5aEb0sWsnDOHqk4dwySOf8PXj\nMzn1yN68kpXH20u2Mm5ITy555JPIvI33ZsktqmDqsnymLstn0c8nU1Fdx8Mzsjl2QHcG9uzMUX27\ncutLX3DW0X1YsnkXPzjrSL73zOeccVRvPs0pYl1BGX+4YgzrC8uYs7aQ754+jG0le0jvlEp1EEa/\neOfLBuu89aUlzFpdwMINxfzrtrP5n3dX8vriPACGZ3Th7R+dSdeO0X/7x2av4/5pa3nq+vGccVQf\nnvpkAx+vLeTyzbsY1rszS4MN1mFbdlVG1r04dydfbtnNdacNIynJWJS7kyv/Oj9uFFxbV09xeTVd\nOqZEdgNeubWEb08YyoINRfz+gzUAXDl+EL+/fEzkeVW1dbz9xVa+fkImBaVV/OmjNUxdls+YQenc\neeFIXs3KY+uuSr514iAuOiGTvJ2V5O2sZMnmXTz9yQZ+/vVR9O2exsqtJVz71AIAPrr9bH74XBa5\nRRXc+43j+Mf8XE4b3ptn523k3aVbeevmMxocUeycY+GGYvqnpzGoZ2eSk4yVW0vILijlsrEDKSyt\nYlneLpbm7WZ9YRmDe3XmpCE9SUk2vvvM56SlJjHt9nNYv6Ocs47q0+y2p9ayQ7nxZvz48S4rK6tN\nlrW7soYxv/qoyccmj+rL7751AjNXF/DorJzIAUBrt5cxeXQ/Ho75Bz1+YDrVtfWRg12O7NMFBwzP\n6MonOYWMH9qLB64ayy/f+bLBBrqOKUmR3eYuHTOAd5Zu3Wt7/9/Fo/n11JWR+0f17UpOQcs+nQek\np7F1976/rj553Xi+3Lqb9YXlvLN0Ky/dOIEJR/Zu0Tpa6tdTV7JgQxHP33AqNz63iJ9fPIoTBvWI\nPO6c42dvLufFhfEX3rh98jE8ML35vYDOOSaDunrHJ01ceemmc4ZTU1fPU59s2Gcbe3ZOpX96J1bl\nl5CSZJEDlcJi/xa9unTg38YN5O/zNnL7ecfwhw/X7HXZI/t3Y/W2pkdqA3t04runD+PdZVvJ3l7W\n4MRhEArQ2FMRtOTvOnZwD1KSjKzcnUwe1Y/pq7Y3O2+3jikt2oh5y7lHcf6x/bl/2lpmri4AgvMM\nHUD5ZVRmd7btrmRnzNksH7/2RJ6bn8vnG4upCb4xpHdK3es55z/+6SS6pqWQ3imVn7y6lDe/2EKn\n1OS417Kx604bynPz48/Tc+7IvpE+tsR/ThzOpJF9eX1RHslJxvED0yODhvFDe9K5Y0qk/n/V+MF8\nkrOjwbfEsGsnDOH5zxoOHv98zTguGTOgxW2JZWaLnHPj46b7GuAQ+ho8ZlAP3vxiC3tq6jj1yN6k\nd0rl4hMym9xY55yjqraed5Zs5fGP1zV7cYjM9LTISBNCJZu8nZUNQruttPSfrjU23HdRws5PMi9n\nB8/O28iV4weTlbuT04f35uxjMhh213sN5uuQkhQ5AMgMmns7fnLnJAb17Mzm4grO+v0sAKb/1zmR\nUswPzjyCzzYUsWJLw/LYn64Yw49fXdpsO8cMSufBq8dx/gNzIiEzsEcnvnniQG6edBQfry3kxn8s\navb5I/t3Iy01Oe7o4PFDe7Jk8664D49Rmd35r/OO4YfPNfw/iP1w++7pw+iWlsKfZ+Y0u16A9245\nk6uf+Ixj+nXjtslHc/rwPpz5u5kN3rt9u3XkX7eexW/eX9Xg20xTwnsgjcrs3uBDtHtaSuRslRA6\noG7MoHReXLgp8prtTd9uHclMT2swmj9xSA+Wb9lNTZ2je1oK547sy1tLooOgbmkpcUf93nTOcL42\nqi9XPD6/yfUkWehYkZImjhbem+MGduc/zjmKn7y6dJ8fFM0Z1LMTJw3tydsxfZg4IoMTBvVgWd4u\nzjkmg+tOG0ZyK0fgh2WAH4jaunoue/RTLghGIpefNIgj+nRhRL9u1NY7bnp+EZNGZDBrTejT9pIx\nA7j1a0eR3qkDJ/9mOgBHZnThoavG8dqizZw2vA9nH9OHlKQkPl5byA+Cf9C5d0zix68sbXCk3Pu3\nnMVFD89l0ogMnvneKQDMXlPAve+t4u/fP4VPs3fw/IJclgVv+OtPG8pPp4wkyeDf/7GI8qpavn/m\nEazKL4mUKe6+cCRlVbWMH9aLv81dz4XHZfJ/Th1yyF7Pllq5tYS6ehcpPcy/+1xueDaLey4ZzdF9\nu9KpQzK3vrSEyaP6sqOsmrOO7kNqchKjMrtHljEvZwfVdfVMHNGXx2avY1NxBfd983iqautYV1DO\nnz5awwXH9ad7WgpTjsvklhe/4PiB6dQ7x4INxZw/uh8TjuzNwzOy+emUEWSmd+KRmdn88aO1nD68\nN//84YQGbX5vWT5zswu54Nj+fLa+iBvOOiKyHWVE/25kdO1IRU0d//3mCt5dupW7LhzJjWcdSXl1\nLQ7olBoq5U1fVcBDV42lZ5cOXPznuazYUsI5x2RwzSmDmXJcJotyiymprGXSyL6U7KnhysfnR0b7\nz33/FN5YnMdbS7by9eMzOW90P74xbiBVtXUYRoeUULlrXWEZO0qreGRWDnOzd5D9mwtJTQ49tqOs\nik9zdlBX7/g0p4hvnTiQmasLmL22kPNH9+P2844hNTmJ3RU1/PAfWVx+4iA6d0wmMz2Nbz0WCs33\nbjmTYwekR16byx75hGF9urC7sobZawp54jsnMW9dEe8vz6d/ehoPXT2OAT3S6JCcxJ6aekbd8wEA\ns38ykac/3cBz83OZe8ckBt1p77kAAAbxSURBVPfqzOuL8nhoRjabiis4fXhvrp0wlMmj+vHFpp2M\nG9Iz0sdNRRWs2lbCacN7c9M/FjFvXREAU47tT59uHXj+s0385dsnsquipsGG9jGD0lmat5vvn3EE\nN559JBPum8F/ThzOHVNCV/u6/6M1PBx8aP76smPZWVFDdkEZeTsr2F1ZExnwnXJELy48rj9dOqSw\nqbiCSSP7clRGV9I7p1JZXcdjc9bx/vJ8Hr56HKMHRN+3B6K5AI9sSDsUPyeddJJrj0r31Ljq2rrI\n/fr6erc8b5crr6pxQ++c6obeOdVV1UQfX51f4v65IHevy1ycW+yW5+2KzP/ywk1ufWGZy95e4pxz\n7tPsQle6p6bZ55dUVru/zMpxP311idtVXt3sfLk7yt1bX+S1qJ/tyZJNO93M1dsT3YyITUXlbuid\nU90bizcf0HJi3yd7s7m43N31+rK9vgecc+6Fz3LdBQ/McdW1dW5PTW3kPbUv5VU1rrB0T4vm3Zct\nOyvc0Dunusl/mt3sPPX19W5dQek+l/Xbf61yQ++c6mrr6l1FVa2bl7OjyWW11LqCUnfv1C/d2m0l\nbndltSvdU+P+Nne9q66tc/X19S5rY5H7cEW+e+GzXLerotptKiqPPHd3ZbWrrYuuq66u3q3dVtJs\n/17L2uxKKpv/XzyYgCzXRKZ+ZUfgLTVrTQEllTVcNnZgopsiB9memjo6piTptLiNOOf488wcLj4h\nkyMz9v+i142XVe9odSnhq6q5EbiXe6EcSpNG9E10E+QQ0cnRmmZm3PK1o9tsWcnK7jbj5X7gIiKi\nABcR8ZYCXETEUwpwERFPKcBFRDylABcR8ZQCXETEUwpwERFPHdIjMc2sEIg/ZVjL9AHiT1Pnr8Op\nP4dTX+Dw6s/h1Bc4vPqzP30Z6pyLOy/0IQ3wA2FmWU0dSuqrw6k/h1Nf4PDqz+HUFzi8+tMWfVEJ\nRUTEUwpwERFP+RTgTyS6AW3scOrP4dQXOLz6czj1BQ6v/hxwX7ypgYuISEM+jcBFRCSGAlxExFNe\nBLiZTTGzNWaWY2Z3Jbo9+2JmT5tZgZmtiJnWy8ymmVl28LtnMN3M7OGgb8vM7MTEtTyemQ02s1lm\nttLMvjSzW4PpvvYnzcwWmtnSoD+/CqYfYWYLgna/bGYdgukdg/s5wePDEtn+pphZspl9YWZTg/s+\n92WjmS03syVmlhVM8/W91sPMXjOz1Wa2ysxOa+u+tPsAN7Nk4FHgQmA0cI2ZjU5sq/bpWWBKo2l3\nATOcc0cDM4L7EOrX0cHPjcBjh6iNLVUL/Ng5NxqYANwcvP6+9qcKONc5NwYYC0wxswnA74AHnHNH\nATuBG4L5bwB2BtMfCOZrb24FVsXc97kvAJOcc2Nj9pH29b32EPCBc24kMIbQ36ht+9LUhTLb0w9w\nGvBhzP27gbsT3a4WtHsYsCLm/hogM7idCawJbv8VuKap+drjD/A2cN7h0B+gM7AYOJXQEXEpjd9z\nwIfAacHtlGA+S3TbY/owKAiCc4GpgPnal6BdG4E+jaZ5914D0oENjV/ftu5Lux+BAwOBzTH384Jp\nvunnnMsPbm8D+gW3velf8JV7HLAAj/sTlByWAAXANGAdsMs5VxvMEtvmSH+Cx3cDvQ9ti/fqQeAO\noD643xt/+wLggI/MbJGZ3RhM8/G9dgRQCDwTlLf+ZmZdaOO++BDghx0X+oj1av9NM+sKvA7c5pwr\niX3Mt/445+qcc2MJjV5PAUYmuEmtYmYXAwXOuUWJbksbOtM5dyKhksLNZnZ27IMevddSgBOBx5xz\n44ByouUSoG364kOAbwEGx9wfFEzzzXYzywQIfhcE09t9/8wslVB4v+CceyOY7G1/wpxzu4BZhMoM\nPcwsJXgots2R/gSPpwNFh7ipzTkDuNTMNgIvESqjPISffQHAObcl+F0AvEnoA9bH91oekOecWxDc\nf41QoLdpX3wI8M+Bo4Mt6x2Aq4F3Etym1ngHuD64fT2hWnJ4+nXBVugJwO6Yr1gJZ2YGPAWscs7d\nH/OQr/3JMLMewe1OhOr5qwgF+eXBbI37E+7n5cDMYOSUcM65u51zg5xzwwj9X8x0zn0bD/sCYGZd\nzKxb+DZwPrACD99rzrltwGYzGxFM+hqwkrbuS6KL/S3cIHARsJZQrfK/E92eFrT3RSAfqCH0SXwD\noVrjDCAbmA70CuY1QnvZrAOWA+MT3f5GfTmT0Ne8ZcCS4Ocij/tzAvBF0J8VwD3B9COBhUAO8CrQ\nMZieFtzPCR4/MtF9aKZfE4GpPvclaPfS4OfL8P+6x++1sUBW8F57C+jZ1n3RofQiIp7yoYQiIiJN\nUICLiHhKAS4i4ikFuIiIpxTgIiKeUoCLiHhKAS4i4qn/D2cZKzkUcVM9AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"AZU2w5Vwdfc7","colab_type":"code","colab":{}},"source":[" def score_regression():\n","\n","    word_encoder = torch.load(word_encoder_path).cuda().eval()\n","    name_encoder = torch.load(name_encoder_path).cuda().eval()\n","    regressor = torch.load(regressor_path2).cuda().eval()\n","\n","    data_set = MovieRatings(word_encoder,name_encoder)\n","    data_loader = DataLoader(data_set, batch_size=1, shuffle=True)\n","    n = len(data_loader)\n","\n","    labels = []\n","    guesses = []\n","    mse = 0\n","    for i, (x, label) in enumerate(data_loader):\n","\n","        pred_y = regressor(x.cuda()).detach().cpu()\n","        guesses.append(pred_y.detach().cpu().item())\n","        labels.append(label.item())\n","\n","\n","    MSE = mean_squared_error(labels,guesses)\n","    RMSE = mean_squared_error(labels,guesses,squared=False)\n","    R2 = r2_score(labels,guesses)\n","\n","    print(\"RSME\", RMSE)\n","    print(\"R2\", R2)\n","    print(\"MSE:\", MSE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-i2jPKCBoLyu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"68ca9f44-b965-4437-a44b-06154d12069c","executionInfo":{"status":"ok","timestamp":1585894121445,"user_tz":360,"elapsed":259485,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}}},"source":["score_regression()"],"execution_count":91,"outputs":[{"output_type":"stream","text":["RSME 1.2603989134936178\n","R2 0.06572310246303348\n","MSE: 1.588605421135892\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uvaG2ZGloWkq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}