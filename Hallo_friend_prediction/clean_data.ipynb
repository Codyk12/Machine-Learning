{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"clean_data.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"omY9xznLJL7J","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"iA4eFQe28JF0","colab_type":"code","outputId":"e80844d2-2b78-44a3-9ca0-200ecd7633d3","executionInfo":{"status":"ok","timestamp":1583019138319,"user_tz":420,"elapsed":25885,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8cDBuJ-KO4HB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QssK0QqdM7iB","colab_type":"code","colab":{}},"source":["# ! pip install iso3166\n","# ! pip install LangCodes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNclewBV8O8F","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import json\n","import re\n","import os\n","# from iso3166 import countries\n","# import langcodes"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EXPpfcOV1hOk","colab_type":"text"},"source":["#### 1.  Identify potential problems with the data:\n"," \n","*   Evaluate the source.\n","    *   We received our data straight from the app database, so it is a pretty reliable source of data.\n","    * The issues with this database are the format with which the data was stored and exported. \n","    * We recieved files which were somewhat like json in nature, but we had to format the json first.\n","    * We then had to import the json files and format them for use with pandas\n","*   What makes you think that?  \n","    *    Data from the app is structured and coerced to be in a certain format with standard answers for all users.\n","*   Evaluate potential biases and other problems with the data (e.g. political data gathered by a particular party trying to get a particular result, or positive pharmaceutical data from the company selling the pharmaceutical) What will you do to deal with these problems?\n","    * Potential bias could come from limited data as the app is new, and the user base for the app could be skewed with the newness of the community. Since users of the app are all learning English, there might also be a potential bias in the variety of users.\n","* Are there missing results or other things that look wrong?\n","    * We dropped a few rows that were stored as test data before the app had users but besides that, if the data has missing values, it is in a reasonable column/entry. The data is in good shape.\n"," \n","#### 2.   What are the strengths and weaknesses of using this data set for answering the questions in the proposal.  If it is not suitable, consider adjusting the questions or acquiring new data.\n","* Strengths:\n","    * The features needed for the task of the project were not extremely complex. Due to this and the fact that the data was collected through automation, minimal errors were encountered in the cleaning process.\n","    * Another strength is that we have data on interactions within a user base of around 300,000. This size will allow us to try out and use a significant amount of features for predicting friendship links and retention.\n","    * The data we currently have is rich for feature extraction. We have text, images, and video encapsulating user interaction which could be mined for information through NLP and Computer Vision. Since there could be many factors influencing why someone is not retained in the app, we may to use these techniques to infer more about the person to better predict and understand why they leave.\n","    \n"," \n","* Weaknesses:\n","    * Time series analysis assumes patterns of past can be used to infer patterns of future. This assumption may not be true with the type of community described by our data. Specifically, our data encompases the initial growth of the community which has many obviously inherent biases such as who initially downloaded the app and how much their intents lined up with the that of the app. Perhaps there exists a different set of initial users that would result in a more successful cascade of naturally forming friendships. Such could render all time series data of the past irrelevant in predicting the strongest factors of friendship in the future.\n","    * In the case of time series data being relevant, there are some data that lack time stamps which could be a significant loss.\n","    * For obvious reasons, personal data such as culture, religion, and interests, often used in community analysis, is not available to us. This will make it more difficult to predict community interactions.\n","    * We lack data on lasting relationships from students to teachers and so we will have to create an evolving model with regards to suggesting teachers to students. \n","    \n","#### 3. Revise any other aspects of your proposal in light of what you have learned from examining the data.\n","* \n","    * After examining the data, it appears that we have enough to still answer the questions we proposed, thus no changes to our proposal are necessary at this time.\n","\n"]},{"cell_type":"code","metadata":{"id":"-jrD_OvT7aAi","colab_type":"code","colab":{}},"source":["# The file directories\n","folder = '/content/drive/My Drive/Hallo_Data/'\n","clean_folder = folder + 'clean_data/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vr1nT68l9Nwu","colab_type":"code","colab":{}},"source":["def clean_json_files():\n","    \"\"\"\n","    This code takes the json file we get from Google BigQuery,\n","    removes the last two entries in the JSON file, and  \n","    converts & writes to a readable json format.\n","    \"\"\"\n","    for f in os.listdir(folder):\n","        # Read in each of the JSON files \n","        if f[-5:] == '.json' and not os.path.exists(\n","                                        clean_folder + f[:-5] + '.json'):\n","            # Replace new lines with commas and remove the \n","            # last two entries of each JSON entry\n","            with open(folder + f, 'r') as infile:\n","                test = infile.read().replace('\\n', ',')\n","                test = test.replace(\",\\\"__error__\\\":[],\\\"__has_error__\\\":false\"\n","                                    , \"\")\n","\n","            # Rewrite the readable JSON files as {name}.json\n","            with open(clean_folder + f[:-5] + '.json', 'x') as outfile:\n","                outfile.write('[' + test[:-1] + ']')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uJR4K81Sw6vM","colab_type":"code","colab":{}},"source":["def get_bans_df(file_name):\n","    \"\"\"\n","    gets the tables of bans\n","    \"\"\"\n","    name = clean_folder+file_name\n","\n","    if os.path.exists(name+\".pkl\"):\n","        return pd.read_pickle(name+\".pkl\")\n","    \n","    # load_from text_file\n","    with open(name+\".txt\" ,'r') as file:\n","        t = file.read().strip().split(\"s~hallotest-53286rL\")\n","    len(t)\n","\n","    # define regex to pull data out\n","    user = re.compile(r\"banned-users\\\" ([^\\s]*) \")\n","    ban = re.compile(r\"bans\\\" ([^\\s]*) \")\n","    desc = re.compile(r\"description  (.*) adminName\")\n","    viol = re.compile(r\"conductViolations  ([^\\s]*) \")\n","\n","    regex = [user,ban,desc,viol]\n","\n","    # define columns\n","    X = []\n","    cols = ['user_id','ban_id','description']\n","\n","    # extract data from each row\n","    for row in t[1:]:\n","        data = []\n","\n","        for reg in regex:\n","            d = user.findall(row)\n","            if d:\n","                data.append(d[0])\n","            else:\n","                data.append(np.nan)\n","\n","        violations = viol.findall(row)\n","        if violations:\n","            data.append(\", \".join(viol.findall(row)))\n","        else:\n","            data.append(np.nan)\n","\n","        X.append(data)\n","\n","    # create dataframe\n","    X = np.array(X)\n","    df = pd.DataFrame(X, columns=cols)\n","    df.to_pickle(name+\".pkl\")\n","\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k3E_kUzG_VND","colab_type":"code","colab":{}},"source":["def get_calls_df(file_name='calls'):\n","    \"\"\"\n","    gets the tables of bans\n","    \"\"\"\n","    name = clean_folder+file_name\n","\n","    if os.path.exists(name+\".pkl\"):\n","        return pd.read_pickle(name+\".pkl\")\n","  \n","    # load_from text_file\n","    with open(name+\".txt\" ,'r') as file:\n","        t = file.read().strip().split(\"s~hallotest-53286r\")\n","\n","    # define regex to pull data out\n","    call_id =re.compile(r\"calls\\\" ([^\\s]*) \")\n","    user = re.compile(r\"fromUserId  ([^\\s]*),\")\n","    to_user = re.compile(r\"toUserId  ([^\\s]*) \")\n","\n","    c_type = re.compile(r\"callType  ([^\\s]*)z\")\n","    status = re.compile(r\"status  ([^\\s]*)z\")\n","\n","    regex = [call_id,user,to_user,c_type,status]\n","\n","    # define columns\n","    X = []\n","    cols = ['call_id','user_id','to_user_id','video','status']\n","\n","    # extract data from each row\n","    for row in t[1:]:\n","        data = []\n","\n","        for reg in regex:\n","            d = reg.findall(row)\n","            # print(d)\n","            if d:\n","                data.append(d[0])\n","            else:\n","                data.append(np.nan)\n","\n","        X.append(data)\n","\n","    # create dataframe\n","    X = np.array(X)\n","    df = pd.DataFrame(X, columns=cols)\n","\n","    # convert to proper types\n","\n","    # 1 if accepted, 0 otherwise\n","    clean_status = lambda x: 1 if x == 'accepted' else 0 \n","    df.status = df.status.apply(clean_status)\n","\n","    # 1 if video, 0 if audio\n","    clean_type = lambda x: 0 if x == 'audio' else 1\n","    df.video = df.video.apply(clean_type)\n","\n","    df.to_pickle(name+\".pkl\")\n","\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N21E4UKlJ5mv","colab_type":"code","colab":{}},"source":["def get_followers_df(file_name,cols):\n","    \"\"\"\n","        loads in follower and followed_livestreams json \n","        file and converts it to pandas\n","    \"\"\"\n","    f = clean_folder +  file_name + '.json'\n","    pkl_file = clean_folder +  file_name + '.pkl'\n","\n","    # check for the pickled file\n","    if os.path.exists(pkl_file):\n","        return pd.read_pickle(pkl_file)\n","\n","    with open(f, 'r') as in_file:\n","        jsn = json.load(in_file)\n","\n","    # extract the follower_ids and streamer_ids\n","    frst,snd = [], []\n","    for row in jsn:\n","        # use built in string regex to get the ideas out the \n","        # json dictionary in string format\n","        temp = row['__key__']['path'].replace(\"\\\"\", \"\")\n","        ids = temp.replace(\"\\'\", \"\").split(\",\")[1::2]\n","        frst.append(ids[0])\n","        snd.append(ids[1])\n","\n","    # create a dataframe out of the follower_ids and streamer_ids\n","    data = np.vstack((frst,snd)).T\n","    df = pd.DataFrame(data,columns=cols).sort_values(cols[0])\n","\n","    # rename columns to appropriate names\n","    df.rename(columns={'Streamer':'streamer_id', \n","                    'Follower':'follower_id'},inplace=True)\n","\n","    df.to_pickle(pkl_file)\n","\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AKxbo2bx1B0J","colab_type":"code","colab":{}},"source":["def get_users_df(file_name=\"users\"):\n","    \"\"\"\n","        loads in user json file and converts it to pandas\n","    \"\"\"\n","    f = clean_folder +  file_name +'.json'\n","    pkl_file = clean_folder +  file_name+'.pkl'\n","\n","    # load if already exists\n","    if os.path.exists(pkl_file):\n","        return pd.read_pickle(pkl_file)\n","\n","    # open json\n","    with open(f, 'r') as in_file:\n","        users = json.load(in_file)\n","\n","    # dump __key__ dict out into user dict\n","    for user in users:\n","        if '__key__' in user:\n","            user.update(user.pop('__key__'))\n","\n","    # get set of all keys\n","    cols = set()\n","    for user in users:\n","        for key in user:\n","            cols.add(key)\n","\n","    #init data\n","    data = {}\n","    for col in cols:\n","        data[col] = []\n","\n","    # extract data from users\n","    for user in users:\n","        for key in data:\n","            if key in user:\n","                data[key].append(user[key])\n","            else:\n","                data[key].append(0)\n","\n","    # format data\n","    df_data = np.array([data[k] for k in data]).T\n","\n","    # enumerate\n","    def enumm(x):\n","        if x == 'expert':\n","            return 4\n","        elif x == 'advanced':\n","            return 3\n","        elif x == 'intermediate':\n","            return 2\n","        elif x == 'beginner':\n","            return 1\n","        else:\n","            return 0\n","\n","    enumm = np.vectorize(enumm)\n","\n","    # export to pandas\n","    usersdf = pd.DataFrame(df_data, columns=list(data.keys()))\n","    usersdf = usersdf.set_index(usersdf['id'])\n","    usersdf.drop('id',axis=1,inplace=True)\n","\n","    usersdf.learningLanguageProficiency = enumm(np.array(usersdf.learningLanguageProficiency.values))\n","\n","    # divided columns into different types for efficient type conversions\n","    floats = ['version', 'followerCount','hopOnRate','points',\n","                'level','learningLanguageProficiency']\n","    dt = ['lastOnline']\n","    bools = ['online','streamer','admin','turnOffRingtone','suspended', \n","                'receiveLevelUpPushNotifications',\n","                'receiveGroupChatInAppNotifications', \n","                'receivePersonalChatInAppNotifications', 'banned', \n","                'receiveLivestreamPushNotifications','darkMode']\n","\n","    # convert to proper type\n","    for col in usersdf.columns:\n","        if col in floats:\n","            usersdf[col] = usersdf[col].astype('float')\n","        elif col in dt:\n","            usersdf[col] = pd.to_datetime(usersdf[col])\n","        elif col in bools:\n","            usersdf[col] = usersdf[col].astype('bool')\n","    \n","    usersdf.drop([],axis=1,inplace=True)\n","\n","    # Rename columns to be more intuitive & drop unwanted columns\n","    cols = {'id':'user_id', \n","            'lastStatusUpdateTime':'last_status_update', \n","            'lastOnline':'last_online', \n","            'nativeCountry':'native_country', \n","            'receiveLivestreamPushNotifications':'livestream_push_notify', \n","            'receivePushNotifications':'push_notify', \n","            'hopOnRate':'hop_on_rate', \n","            'learningLanguageProficiency':'english_proficiency', \n","            'nativeLanguage':'native_lang', \n","            'followerCount':'follower_count', \n","            'coinBalance':'coin_balance'}\n","\n","    usersdf.rename(columns=cols, inplace=True)\n","\n","    # Rename the index to match other tables\n","    usersdf.index = usersdf.index.rename('user_id')\n","    \n","    # Drop columns for privacy and irrelevant factors\n","    usersdf.drop(columns=['firstName', 'lastName', 'userName', 'name',\n","                            'path','app','profilePicture','profilePictureOld',\n","                            'profilePictureIcon', 'profilePictureIconOld', \n","                            'namespace','version','kind','online', \n","                            'learningLanguage', \n","                            'busy', 'darkMode', 'doNotDisturb', 'livestream', \n","                            'turnOffRingtone', 'searchName', \n","                            'receiveLevelUpInAppNotifications', \n","                            'receiveFollowingPushNotifications', \n","                            'receiveGroupChatPushNotifications', \n","                            'receiveGroupChatInAppNotifications', \n","                            'receiveLevelUpPushNotifications', \n","                            'receivePersonalChatPushNotifications', \n","                            'receivePersonalChatInAppNotifications', \n","                            'receiveInAppNotifications', \n","                            'receiveLivestreamInAppNotifications', \n","                            'receiveFollowingInAppNotifications'], \n","                inplace=True)\n","\n","    # Replace 0's with NaN's and NaT's\n","    usersdf['native_country'] = usersdf['native_country'].replace(0, np.nan)\n","    usersdf['last_status_update'] =  pd.to_datetime(usersdf['last_status_update'].replace(0, pd.NaT))\n","    usersdf.loc[(usersdf.last_online == usersdf.iloc[0].last_online).values,'last_online'] = pd.NaT\n","    usersdf = usersdf[usersdf.index != 0]\n","\n","    # Convert country abbreviations to full names\n","    def get_country_name(x):\n","        try:\n","            return countries.get(x).name\n","        except:\n","        # These are non-standard country codes\n","            if x == 'an':\n","                return 'Andorra'\n","            else:\n","                return x.capitalize()\n","    usersdf.native_country = usersdf.native_country[usersdf.native_country.notna()].apply(get_country_name)\n","    usersdf.native_country = usersdf.native_country.astype(str)\n","\n","    # Convert language abbreviations to full names\n","    usersdf.native_lang = usersdf.native_lang.replace(0, np.nan)\n","    usersdf.native_lang = usersdf.native_lang[usersdf.native_lang.notna()].apply(lambda x: langcodes.get(x).language_name())\n","\n","    # Convert Bio to participation flag\n","    usersdf['bio'] = usersdf['bio'].apply(lambda x: False if x in [0, '\\n', ''] else True)\n","\n","    # Convert other columns to proper data types\n","    usersdf.english_proficiency = usersdf.english_proficiency.astype(int)\n","    usersdf.follower_count = usersdf.follower_count.astype(int)\n","    usersdf.coin_balance = usersdf.coin_balance.astype(int)\n","    usersdf['push_notify'] = usersdf['push_notify'].astype(bool)\n","\n","    # Save cleaned data to pickle file\n","    usersdf.to_pickle(pkl_file)\n","    # usersdf[usersdf['streamer'] == True].to_pickle(clean_folder + \"streamers.pkl\")\n","    # usersdf[usersdf['streamer'] == False].to_pickle(clean_folder + \"non_streamers.pkl\")\n","\n","    return usersdf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWV8HtoPLTMK","colab_type":"code","colab":{}},"source":["def get_livestreams_df(file_name=\"livestream_recording\"):\n","    \"\"\"\n","    Loads in livestream recording data, and converts it to a pandas DataFrame\n","    \"\"\"\n","    # Define the default files for working in colab.\n","    file = clean_folder + file_name + '.json'\n","    pkl_file = clean_folder + file_name +'.pkl'\n","\n","    # Load the file if if already exists:\n","    if os.path.exists(pkl_file):\n","        return pd.read_pickle(pkl_file)\n","\n","    # Open json file\n","    with open(file, 'r') as in_file:\n","        livestream_recording = json.load(in_file)\n","\n","    # Find all the unique columns in the dataset\n","    unique_cols = set()\n","    bools = ['uploaded', 'hidden', 'premium', 'currentlyLive', 'uploaded']\n","    floats = ['views', 'startTimestamp']\n","\n","    # Move data from '__key__' to livestream_recording dict\n","    for k in range(len(livestream_recording)):\n","        livestream_recording[k].update(livestream_recording[k].pop('__key__'))\n","        unique_cols.update(livestream_recording[k].keys())\n","\n","    # Add nans where there is missing data\n","    for k in range(len(livestream_recording)):\n","        for key in unique_cols:\n","            if key not in livestream_recording[k].keys():\n","                if key in bools:\n","                    livestream_recording[k][key] = 0\n","                else:\n","                    livestream_recording[k][key] = np.nan\n","\n","    # Cast our data as a dataframe\n","    lsr_df = pd.DataFrame(livestream_recording)\n","\n","    # Extract non-redundant information about the streamer, then drop the column\n","    def get_fn(x):\n","        if isinstance(x,dict):\n","            return x.get('firstName')\n","        else: \n","            return x\n","\n","    lsr_df['streamerFirstName'] = lsr_df[lsr_df['streamer'].notna()].streamer.apply(get_fn)\n","    lsr_df.drop(columns='streamer', inplace=True)\n","\n","    # Change the column datatypes \n","    for column in lsr_df.columns:\n","        if column in bools:\n","            # Fill in nan values as False\n","            lsr_df[column] = lsr_df[column].astype(bool)\n","        elif column in floats:\n","            lsr_df[column] = lsr_df[column].astype(float)        \n","        else:\n","            lsr_df[column] = lsr_df[column].astype(str)\n","    \n","    # Convert the timestamp column to a datetime object\n","    lsr_df.startTimestamp = pd.to_datetime(lsr_df.startTimestamp, unit='ms')\n","    \n","    # Drop the unwanted columns\n","    cols_to_drop = lsr_df.columns & ['namespace', 'app', 'path', 'kind', \n","                                     'filename', 'videoUrl', 'currentlyLive', \n","                                     'thumbnail', 'agoraRecordingUid', \n","                                     'agoraResourceId', 'agoraRecordingId']\n","    lsr_df.drop(columns=cols_to_drop, inplace=True)\n","    lsr_df.replace('nan', np.nan, inplace=True)\n","\n","    # This is for a livestream table\n","    if 'streamerId' not in lsr_df.columns:\n","        lsr_df['streamerId'] = lsr_df.name\n","        lsr_df.drop('name',axis=1,inplace=True)\n","\n","        # drop irrelevant columns\n","        ls.drop(columns=['streamerFirstName',\n","                         'autoRecord',\n","                         'recordingId'],inplace=True)\n","\n","        # convert to proper types\n","        ls.viewers = ls.viewers.astype(int)\n","\n","    # This is for livestream recording table\n","    else:\n","        lsr.rename(columns={'name':'stream_id'}, inplace=True)\n","        lsr.views = lsr.views.astype(int)\n","\n","    lsr.rename(columns={'streamerId':'streamer_id', \n","                        'startTimestamp':'start_time'}, inplace=True)\n","    \n","    lsr.drop(columns=['streamerFirstName'], inplace=True)\n","    \n","    lsr_df = lsr_df[lsr_df.streamerId.notna()]\n","\n","    lsr_df.to_pickle(pkl_file)\n","    return lsr_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qdi3O1cdAoAC","colab_type":"code","colab":{}},"source":["def get_friends_df(file_name):\n","    \"\"\"\n","    Loads in friends_table data, and converts it to a pandas DataFrame\n","    \"\"\"\n","    # Define the default files for working in colab.\n","    file = clean_folder + file_name + '.json'\n","    pkl_file = clean_folder + file_name +'.pkl'\n","  \n","    # Load the file if if already exists:\n","    if os.path.exists(pkl_file):\n","        return pd.read_pickle(pkl_file)\n","\n","    else:\n","        # Open json file\n","        with open(file, 'r') as in_file:\n","            friends = json.load(in_file)\n","        \n","        # get the unique columns for the friends table\n","        unique_cols = set()\n","        bools = []\n","        for k in range(len(friends)):\n","            friends[k].update(friends[k].pop('__key__'))\n","            unique_cols.update(friends[k].keys())\n","\n","            # loop over the unique columns \n","            for k in range(len(friends)):\n","                for key in unique_cols:\n","\n","                    # set the dictionary appropriately \n","                    if key not in friends[k].keys():\n","                        if key in bools:\n","                            friends[k][key] = 0\n","                        else:\n","                            friends[k][key] = np.nan\n","    \n","\n","    # Cast our data as a dataframe & pickle it\n","    friends_df = pd.DataFrame(friends)\n","    friend_pattern = re.compile(\"\\\"[\\w]+\\\", \\\"([\\w]+)\\\", \\\"[\\w]+\\\", \\\"[\\w]+\\\"\")\n","    friends_df['friend_id'] = friends_df['path'].apply(lambda x: friend_pattern.findall(x)[0])\n","    friends_df.rename(columns={'id':'user_id'}, inplace=True)\n","\n","    # Drop columns for privacy & redundancy\n","    friends_df.drop(columns=['app', 'namespace', 'profilePicture', \n","                                'kind', 'name', 'path', 'firstName'], inplace=True)\n","    friends_df.to_pickle(pkl_file)\n","\n","    return friends_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUEwwoKm6nYB","colab_type":"code","colab":{}},"source":["def loadpandas():\n","    \"\"\"\n","    This loads all the cleaned json files into pandas, \n","    saves them and returns them\n","    \"\"\"\n","    # Run our function to get readable json files.\n","    clean_json_files()\n","\n","    all_tables = {}\n","\n","    all_tables['calls'] = get_calls_df('calls')\n","    all_tables['bans'] = get_bans_df('bans')\n","    all_tables['users'] = get_users_df()\n","    all_tables['lsr'] = get_livestreams_df(\"livestream_recording\")\n","    all_tables['ls'] = get_livestreams_df(\"livestream\")\n","    all_tables['friends'] = get_friends_df(\"friends\")\n","    all_tables['followers'] = get_followers_df(\"followers\",\n","                                               [\"Streamer\",\"Follower\"])\n","\n","    # Followed Livestreams and Followers appear to have similar information, \n","    # (may have use for it in future)\n","    # all_tables['fl'] = get_followers_df(\"followed_livestream\",\n","    #                                    [\"Users\",\"Followed_Livestreams\"]) \n","    return all_tables"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQCdomr-695W","colab_type":"code","colab":{}},"source":["tables = loadpandas()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SpfcXy-v6Tol","colab_type":"code","outputId":"af993123-c6dd-4512-a72e-2eeb1cd01d75","executionInfo":{"status":"ok","timestamp":1582746596173,"user_tz":420,"elapsed":427,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["#users table\n","tables['users'].sample(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>last_status_update</th>\n","      <th>native_country</th>\n","      <th>native_lang</th>\n","      <th>points</th>\n","      <th>streamer</th>\n","      <th>follower_count</th>\n","      <th>admin</th>\n","      <th>coin_balance</th>\n","      <th>bio</th>\n","      <th>last_online</th>\n","      <th>english_proficiency</th>\n","      <th>banned</th>\n","      <th>hop_on_rate</th>\n","      <th>livestream_push_notify</th>\n","      <th>suspended</th>\n","      <th>push_notify</th>\n","      <th>level</th>\n","    </tr>\n","    <tr>\n","      <th>user_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Rs0uTm707DakujYzirmkpfMRP8F3</th>\n","      <td>NaT</td>\n","      <td>Viet Nam</td>\n","      <td>Chinese</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>2019-10-01 09:21:41.567000+00:00</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>kjWrtwQVFyfhs6R66ZvSOdtjguw1</th>\n","      <td>NaT</td>\n","      <td>Malaysia</td>\n","      <td>Malay</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>2019-11-13 17:30:25.998000+00:00</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>005c55t0uiXz0s91RL02ilDQNkN2</th>\n","      <td>NaT</td>\n","      <td>Viet Nam</td>\n","      <td>English</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>NaT</td>\n","      <td>2</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>BDjVk6E7GKRdT8QaMhiZsGOn3Dh1</th>\n","      <td>NaT</td>\n","      <td>Morocco</td>\n","      <td>Arabic</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>NaT</td>\n","      <td>2</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>onJvumDdAXZNcE2ob78Y0vYEkCx1</th>\n","      <td>2019-06-19 17:49:07.933000+00:00</td>\n","      <td>Morocco</td>\n","      <td>Arabic</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>2019-06-19 17:49:07.933000+00:00</td>\n","      <td>2</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           last_status_update  ... level\n","user_id                                                        ...      \n","Rs0uTm707DakujYzirmkpfMRP8F3                              NaT  ...   0.0\n","kjWrtwQVFyfhs6R66ZvSOdtjguw1                              NaT  ...   0.0\n","005c55t0uiXz0s91RL02ilDQNkN2                              NaT  ...   0.0\n","BDjVk6E7GKRdT8QaMhiZsGOn3Dh1                              NaT  ...   0.0\n","onJvumDdAXZNcE2ob78Y0vYEkCx1 2019-06-19 17:49:07.933000+00:00  ...   0.0\n","\n","[5 rows x 17 columns]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"JcdYewk5Cpcr","colab_type":"code","outputId":"ffdcafef-50b0-4535-bc65-c0f7640bf38b","executionInfo":{"status":"ok","timestamp":1582746619551,"user_tz":420,"elapsed":562,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":148}},"source":["tables['users'][tables['users'].index == '008Z1Zo7FTOGEsI9CFnHeMKoeF03']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>last_status_update</th>\n","      <th>native_country</th>\n","      <th>native_lang</th>\n","      <th>points</th>\n","      <th>streamer</th>\n","      <th>follower_count</th>\n","      <th>admin</th>\n","      <th>coin_balance</th>\n","      <th>bio</th>\n","      <th>last_online</th>\n","      <th>english_proficiency</th>\n","      <th>banned</th>\n","      <th>hop_on_rate</th>\n","      <th>livestream_push_notify</th>\n","      <th>suspended</th>\n","      <th>push_notify</th>\n","      <th>level</th>\n","    </tr>\n","    <tr>\n","      <th>user_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>008Z1Zo7FTOGEsI9CFnHeMKoeF03</th>\n","      <td>2019-08-14 12:08:18.933000+00:00</td>\n","      <td>India</td>\n","      <td>Hindi</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>NaT</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           last_status_update  ... level\n","user_id                                                        ...      \n","008Z1Zo7FTOGEsI9CFnHeMKoeF03 2019-08-14 12:08:18.933000+00:00  ...   0.0\n","\n","[1 rows x 17 columns]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"3d145855-5ed8-4f3f-cfe9-82292879e3a5","executionInfo":{"status":"ok","timestamp":1574730071089,"user_tz":420,"elapsed":329,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"id":"jhg2j5MDojq6","colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["# followers table\n","tables['followers']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>streamer_id</th>\n","      <th>follower_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>23413</th>\n","      <td>1Z6XYA1vgQXiTQTRpNhzXsj42Qk1</td>\n","      <td>OA9PVrOYHkdPAm56vDtzncMHjL52</td>\n","    </tr>\n","    <tr>\n","      <th>17847</th>\n","      <td>1Z6XYA1vgQXiTQTRpNhzXsj42Qk1</td>\n","      <td>IGKj8DE5ltUolZy6GCKvfMkZX5q2</td>\n","    </tr>\n","    <tr>\n","      <th>49504</th>\n","      <td>1Z6XYA1vgQXiTQTRpNhzXsj42Qk1</td>\n","      <td>q662PDTGVIffYfYPLtp4W4OoMu13</td>\n","    </tr>\n","    <tr>\n","      <th>36656</th>\n","      <td>1Z6XYA1vgQXiTQTRpNhzXsj42Qk1</td>\n","      <td>cJyMU6rTsieycSKpWzGHxRQ6eAu1</td>\n","    </tr>\n","    <tr>\n","      <th>26566</th>\n","      <td>1Z6XYA1vgQXiTQTRpNhzXsj42Qk1</td>\n","      <td>RPMxeneyuIhhe9qQrJnD8QUmCbB2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1054</th>\n","      <td>z8N27DEBoSUCjCticAaCzLSkjC22</td>\n","      <td>17RqQClvHtcr0Ajwh1b2JX32YJy2</td>\n","    </tr>\n","    <tr>\n","      <th>24505</th>\n","      <td>z8N27DEBoSUCjCticAaCzLSkjC22</td>\n","      <td>PAxOlcf267OEk5kYB9tdLjePBqf1</td>\n","    </tr>\n","    <tr>\n","      <th>11373</th>\n","      <td>z8N27DEBoSUCjCticAaCzLSkjC22</td>\n","      <td>BvUM9ktm8bfX33oLsIpaoWDbtWA3</td>\n","    </tr>\n","    <tr>\n","      <th>27070</th>\n","      <td>z8N27DEBoSUCjCticAaCzLSkjC22</td>\n","      <td>RyvqCZvQ0bMkkzyJVBUXWyRbHRk1</td>\n","    </tr>\n","    <tr>\n","      <th>19067</th>\n","      <td>z8N27DEBoSUCjCticAaCzLSkjC22</td>\n","      <td>JfPl5VWjUuNSdlKSkbv3LZo3hSD2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>59187 rows √ó 2 columns</p>\n","</div>"],"text/plain":["                         streamer_id                    follower_id\n","23413   1Z6XYA1vgQXiTQTRpNhzXsj42Qk1   OA9PVrOYHkdPAm56vDtzncMHjL52\n","17847   1Z6XYA1vgQXiTQTRpNhzXsj42Qk1   IGKj8DE5ltUolZy6GCKvfMkZX5q2\n","49504   1Z6XYA1vgQXiTQTRpNhzXsj42Qk1   q662PDTGVIffYfYPLtp4W4OoMu13\n","36656   1Z6XYA1vgQXiTQTRpNhzXsj42Qk1   cJyMU6rTsieycSKpWzGHxRQ6eAu1\n","26566   1Z6XYA1vgQXiTQTRpNhzXsj42Qk1   RPMxeneyuIhhe9qQrJnD8QUmCbB2\n","...                              ...                            ...\n","1054    z8N27DEBoSUCjCticAaCzLSkjC22   17RqQClvHtcr0Ajwh1b2JX32YJy2\n","24505   z8N27DEBoSUCjCticAaCzLSkjC22   PAxOlcf267OEk5kYB9tdLjePBqf1\n","11373   z8N27DEBoSUCjCticAaCzLSkjC22   BvUM9ktm8bfX33oLsIpaoWDbtWA3\n","27070   z8N27DEBoSUCjCticAaCzLSkjC22   RyvqCZvQ0bMkkzyJVBUXWyRbHRk1\n","19067   z8N27DEBoSUCjCticAaCzLSkjC22   JfPl5VWjUuNSdlKSkbv3LZo3hSD2\n","\n","[59187 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"6NCodRvS6_rs","colab_type":"code","outputId":"572af498-2bcb-45d4-8779-36ec3a6b5280","executionInfo":{"status":"ok","timestamp":1574471447273,"user_tz":420,"elapsed":1260,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["# live stream recordings table\n","tables['lsr'].sample(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>views</th>\n","      <th>stream_id</th>\n","      <th>premium</th>\n","      <th>start_time</th>\n","      <th>streamer_id</th>\n","      <th>title</th>\n","      <th>uploaded</th>\n","      <th>category</th>\n","      <th>hidden</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>90</th>\n","      <td>2339</td>\n","      <td>PueZCjtPkI5jxrs390yk</td>\n","      <td>False</td>\n","      <td>2019-09-11 03:52:22.722</td>\n","      <td>71vksrN1EHancbxxtAnkhPI4VBj1</td>\n","      <td>How to pronounce the 10 MOST COMMON WORDS in E...</td>\n","      <td>True</td>\n","      <td>Speaking</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>30</td>\n","      <td>F5WPeQDlFI0tZGd4l7MK</td>\n","      <td>False</td>\n","      <td>2019-11-08 05:33:17.283</td>\n","      <td>yOiAC2dilQPYLLoOJMa4v3dwLek2</td>\n","      <td>testing</td>\n","      <td>False</td>\n","      <td>Culture</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2</td>\n","      <td>yuhFmqEItDXzQkfex9OL</td>\n","      <td>False</td>\n","      <td>NaT</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>False</td>\n","      <td>NaN</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>1511</td>\n","      <td>8JIukfdrBHB3OFCDcEhz</td>\n","      <td>False</td>\n","      <td>2019-08-27 17:40:01.831</td>\n","      <td>JdopYYS74rQZIAjSAsRpwWBrfgl1</td>\n","      <td>Culture topic: Let‚Äôs talk about families! üë®‚Äçüë©‚Äç...</td>\n","      <td>True</td>\n","      <td>Culture</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>154</th>\n","      <td>1732</td>\n","      <td>4BBtzR2OvPr2n37hyjRc</td>\n","      <td>False</td>\n","      <td>2019-09-20 22:25:20.369</td>\n","      <td>WWPLsmHyboSCClOJZPDR28AB9ct1</td>\n","      <td>St. Lucia Honeymoon Part II</td>\n","      <td>True</td>\n","      <td></td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     views             stream_id  premium  ... uploaded  category hidden\n","90    2339  PueZCjtPkI5jxrs390yk    False  ...     True  Speaking  False\n","32      30  F5WPeQDlFI0tZGd4l7MK    False  ...    False   Culture   True\n","16       2  yuhFmqEItDXzQkfex9OL    False  ...    False       NaN  False\n","49    1511  8JIukfdrBHB3OFCDcEhz    False  ...     True   Culture  False\n","154   1732  4BBtzR2OvPr2n37hyjRc    False  ...     True            False\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"w4_OZ3QB7AmK","colab_type":"code","outputId":"271d8ff7-255b-4820-d1ec-d835c9b28e0e","executionInfo":{"status":"ok","timestamp":1574471453570,"user_tz":420,"elapsed":1163,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#live streams\n","tables['ls'].sample(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>category</th>\n","      <th>premium</th>\n","      <th>title</th>\n","      <th>start_time</th>\n","      <th>viewers</th>\n","      <th>streamer_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>Culture</td>\n","      <td>False</td>\n","      <td>TEST - resolving some issues</td>\n","      <td>2019-08-14 21:52:50.663</td>\n","      <td>13</td>\n","      <td>EcPAITqb2fdG66Gz1EQQpyiqEkM2</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Culture</td>\n","      <td>False</td>\n","      <td>testing again</td>\n","      <td>2019-11-13 04:34:26.649</td>\n","      <td>3</td>\n","      <td>yOiAC2dilQPYLLoOJMa4v3dwLek2</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Culture</td>\n","      <td>False</td>\n","      <td>Sunday Selection: Abhay - Nature + Food Idioms</td>\n","      <td>2019-11-10 16:59:35.387</td>\n","      <td>147</td>\n","      <td>WWPLsmHyboSCClOJZPDR28AB9ct1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Culture</td>\n","      <td>False</td>\n","      <td>testing</td>\n","      <td>2019-10-22 19:44:40.117</td>\n","      <td>0</td>\n","      <td>UxV75ClA1gZFNUySPJSm2GVqg243</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Culture</td>\n","      <td>False</td>\n","      <td>100 Questions</td>\n","      <td>2019-11-03 14:12:23.907</td>\n","      <td>29</td>\n","      <td>z8N27DEBoSUCjCticAaCzLSkjC22</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    category  premium  ... viewers                   streamer_id\n","1    Culture    False  ...      13  EcPAITqb2fdG66Gz1EQQpyiqEkM2\n","19   Culture    False  ...       3  yOiAC2dilQPYLLoOJMa4v3dwLek2\n","18   Culture    False  ...     147  WWPLsmHyboSCClOJZPDR28AB9ct1\n","5    Culture    False  ...       0  UxV75ClA1gZFNUySPJSm2GVqg243\n","8    Culture    False  ...      29  z8N27DEBoSUCjCticAaCzLSkjC22\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"qjOVWKNaA9Mj","colab_type":"code","outputId":"b5674268-5c90-4bd5-f2cf-0fd2673ee2af","executionInfo":{"status":"ok","timestamp":1574471454790,"user_tz":420,"elapsed":450,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# friends table\n","tables['friends'].sample(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>friend_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>57271</th>\n","      <td>SEOTyCdlrFc1zZI2uYQggHd42e22</td>\n","      <td>gqKHroIl2xTVu8zr88VuqXcvcwt1</td>\n","    </tr>\n","    <tr>\n","      <th>325190</th>\n","      <td>vf2tlq4EqhRDdyY7qzckpx8LnMP2</td>\n","      <td>7PKXkPNT0XZfoZcIhc7bKQqgqrc2</td>\n","    </tr>\n","    <tr>\n","      <th>60807</th>\n","      <td>kBSTShIf5OhXnIAQZ5uL63QANSI3</td>\n","      <td>WnaP4cDtiqQDPrp5TaB0QHzMBEu1</td>\n","    </tr>\n","    <tr>\n","      <th>304682</th>\n","      <td>usygtUYMc4cNtZXTBr3t8Y6CLcy1</td>\n","      <td>MBsHWSKZVNdkrIsMLvxQyL6Fdqy2</td>\n","    </tr>\n","    <tr>\n","      <th>76696</th>\n","      <td>jx3UTcrKWTNJLri6ynVWfTYV66i1</td>\n","      <td>Sa4B1yTO74TcxRyYZngTddrv78s1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                             user_id                     friend_id\n","57271   SEOTyCdlrFc1zZI2uYQggHd42e22  gqKHroIl2xTVu8zr88VuqXcvcwt1\n","325190  vf2tlq4EqhRDdyY7qzckpx8LnMP2  7PKXkPNT0XZfoZcIhc7bKQqgqrc2\n","60807   kBSTShIf5OhXnIAQZ5uL63QANSI3  WnaP4cDtiqQDPrp5TaB0QHzMBEu1\n","304682  usygtUYMc4cNtZXTBr3t8Y6CLcy1  MBsHWSKZVNdkrIsMLvxQyL6Fdqy2\n","76696   jx3UTcrKWTNJLri6ynVWfTYV66i1  Sa4B1yTO74TcxRyYZngTddrv78s1"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"jMYrd7X_Jrid","colab_type":"code","outputId":"ddcae2f1-4fdb-42c2-e52f-924bca6108ee","executionInfo":{"status":"ok","timestamp":1582746447699,"user_tz":420,"elapsed":382,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["# bans table\n","tables['bans']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>ban_id</th>\n","      <th>description</th>\n","      <th>vilolations</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>008Z1Zo7FTOGEsI9CFnHeMKoeF03</td>\n","      <td>lNmT1N4tHsaEYS2BCtqt</td>\n","      <td>bad person z</td>\n","      <td>hateful_conduct_and_harassmentz4, distributing...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>02MPFMtPUsZs9qvk2aDvGfXletN2</td>\n","      <td>z3J3pwYVBrlMWaCbRO48</td>\n","      <td>byeeez</td>\n","      <td>+nudity_pornography_and_other_sexual_contentz</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>053x3c4VkxUmk0mOXfenNIAufHA2</td>\n","      <td>3ByAwlbYQv600nK8OiVj</td>\n","      <td>byeeeez</td>\n","      <td>&amp;spam_scams_and_other_malicious_conductz</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>054yvZffP2UrFUBJ4rVC7rNXWqy1</td>\n","      <td>nLAO6qNwVh7eUQvszFxP</td>\n","      <td>nan</td>\n","      <td>distributing_advertisementsz?, &amp;spam_scams_and...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>09GJJt9c84PLBIR93MeAs9jmmc53</td>\n","      <td>XE5Ub06HUWGw0VAbcpNj</td>\n","      <td>bitcoinz</td>\n","      <td>&amp;spam_scams_and_other_malicious_conductz4, dis...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1574</th>\n","      <td>zfuZVj3NBxbdI3HqIRfJVgPLuQ83</td>\n","      <td>hPpeleT7UCBv8nC84kkS</td>\n","      <td>byeeez</td>\n","      <td>+nudity_pornography_and_other_sexual_contentz</td>\n","    </tr>\n","    <tr>\n","      <th>1575</th>\n","      <td>zjVNzej1xeVpvDlLDyQTRecDn7D2</td>\n","      <td>oC02SNEU2okaRbT6xr8A</td>\n","      <td>nan</td>\n","      <td>+nudity_pornography_and_other_sexual_contentz</td>\n","    </tr>\n","    <tr>\n","      <th>1576</th>\n","      <td>zl77CIVCIpdY55CwiP7uUPDw4At2</td>\n","      <td>zkMFZMyCq8muVAeZ8yRQ</td>\n","      <td>nan</td>\n","      <td>hateful_conduct_and_harassmentzn</td>\n","    </tr>\n","    <tr>\n","      <th>1577</th>\n","      <td>zlUWQz4e25OMElNuWwMkTHGSYe72</td>\n","      <td>CK2uEk3YOWHwvzaTnytV</td>\n","      <td>byeeeez</td>\n","      <td>&amp;spam_scams_and_other_malicious_conductz</td>\n","    </tr>\n","    <tr>\n","      <th>1578</th>\n","      <td>zu4k4vYlZ9SRyTFjM95Kxq00IbA2</td>\n","      <td>AMfbNg8oLyQPLmcdmro8</td>\n","      <td>byeeez</td>\n","      <td>+nudity_pornography_and_other_sexual_contentz</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1579 rows √ó 4 columns</p>\n","</div>"],"text/plain":["                           user_id  ...                                        vilolations\n","0     008Z1Zo7FTOGEsI9CFnHeMKoeF03  ...  hateful_conduct_and_harassmentz4, distributing...\n","1     02MPFMtPUsZs9qvk2aDvGfXletN2  ...      +nudity_pornography_and_other_sexual_contentz\n","2     053x3c4VkxUmk0mOXfenNIAufHA2  ...           &spam_scams_and_other_malicious_conductz\n","3     054yvZffP2UrFUBJ4rVC7rNXWqy1  ...  distributing_advertisementsz?, &spam_scams_and...\n","4     09GJJt9c84PLBIR93MeAs9jmmc53  ...  &spam_scams_and_other_malicious_conductz4, dis...\n","...                            ...  ...                                                ...\n","1574  zfuZVj3NBxbdI3HqIRfJVgPLuQ83  ...      +nudity_pornography_and_other_sexual_contentz\n","1575  zjVNzej1xeVpvDlLDyQTRecDn7D2  ...      +nudity_pornography_and_other_sexual_contentz\n","1576  zl77CIVCIpdY55CwiP7uUPDw4At2  ...                   hateful_conduct_and_harassmentzn\n","1577  zlUWQz4e25OMElNuWwMkTHGSYe72  ...           &spam_scams_and_other_malicious_conductz\n","1578  zu4k4vYlZ9SRyTFjM95Kxq00IbA2  ...      +nudity_pornography_and_other_sexual_contentz\n","\n","[1579 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"8fLPT9Thz9NX","colab_type":"code","outputId":"230e8db4-d645-4ac2-ef20-be4cbf4d48fd","executionInfo":{"status":"ok","timestamp":1574797659136,"user_tz":420,"elapsed":665,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["# calls table\n","tables['calls'].sort_values(by=\"user_id\")"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>call_id</th>\n","      <th>user_id</th>\n","      <th>to_user_id</th>\n","      <th>video</th>\n","      <th>status</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>42</th>\n","      <td>870ua3cyBeUDm2Jdy1oe</td>\n","      <td>0M5C9cBfiuhXspMtbwhnKR4rbQw2z</td>\n","      <td>dj8K09c8UcanZQ3i4vm1cMwVdmj2z</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>125</th>\n","      <td>KOuAO9uJhltAaXM4lFX8</td>\n","      <td>15mlRziqMzg8Evf2LYk2qw5Jxyt2z</td>\n","      <td>6BF3i7IX0OSHyaIQjyNJwlGzLD62z</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>BeH0tTZtTf6ltVZTfrdp</td>\n","      <td>2FRjyQo2VHWVOawPTU0GK7eBnLQ2z</td>\n","      <td>AWZ9daFsisbctT6WT0m9iKuXX9U2z</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>74</th>\n","      <td>Ca5R3E0bkIH3bGHR4BEx</td>\n","      <td>3D3pt31jKTYozCRvQETNZeXipHX2z</td>\n","      <td>FsT5zhML4GStl0uDCSNw9oE6z1o2z</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>6GKKHg85EtyriESiU6Xy</td>\n","      <td>3qjlQiLuoIgOFSQ2pEsTMb0CpCp2z</td>\n","      <td>KLoKL9uAblPyGfvNCKJ0TYbel2E2z</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>117</th>\n","      <td>J2N0ygphgik4uJXym0u2</td>\n","      <td>vyIJlsZi9Zdb41ZZmoOq1uV2PQi2z</td>\n","      <td>X5JZVwOplwgumj5Aw7jBcxLiR9I3z</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>88</th>\n","      <td>E4WdWMygerJCICvbQOed</td>\n","      <td>wbvgDPxwKxP1VTumyTRYSNKCO7d2z</td>\n","      <td>WTd22jgNeZaXot60yVMBx2J5vY73z</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>F2U6BPgcIq2CqAPbSzsl</td>\n","      <td>wk4OnZKqMXYHHWj3a8fPMEjysev2z</td>\n","      <td>2Qx5W2tKEphk0AdPwdMXN8uRYb52z</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>BOVOnIXVOVpMGardajIi</td>\n","      <td>xwwHXXWvDWcy5lIE8yNmcoiooZx2z</td>\n","      <td>WlNxvd4FMahtwICPWxyiMDzhh8m1z</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>7SKko21jIuhhaKhmLd8d</td>\n","      <td>zwg96dRoh1PPCjucuXJYPxAKQlg1z</td>\n","      <td>NiyjZZdrTKTwg25QQsxquWhy9vH3z</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>134 rows √ó 5 columns</p>\n","</div>"],"text/plain":["                  call_id                        user_id  ... video  status\n","42   870ua3cyBeUDm2Jdy1oe  0M5C9cBfiuhXspMtbwhnKR4rbQw2z  ...     1       0\n","125  KOuAO9uJhltAaXM4lFX8  15mlRziqMzg8Evf2LYk2qw5Jxyt2z  ...     0       1\n","65   BeH0tTZtTf6ltVZTfrdp  2FRjyQo2VHWVOawPTU0GK7eBnLQ2z  ...     1       0\n","74   Ca5R3E0bkIH3bGHR4BEx  3D3pt31jKTYozCRvQETNZeXipHX2z  ...     1       1\n","31   6GKKHg85EtyriESiU6Xy  3qjlQiLuoIgOFSQ2pEsTMb0CpCp2z  ...     0       0\n","..                    ...                            ...  ...   ...     ...\n","117  J2N0ygphgik4uJXym0u2  vyIJlsZi9Zdb41ZZmoOq1uV2PQi2z  ...     0       1\n","88   E4WdWMygerJCICvbQOed  wbvgDPxwKxP1VTumyTRYSNKCO7d2z  ...     0       0\n","96   F2U6BPgcIq2CqAPbSzsl  wk4OnZKqMXYHHWj3a8fPMEjysev2z  ...     0       1\n","64   BOVOnIXVOVpMGardajIi  xwwHXXWvDWcy5lIE8yNmcoiooZx2z  ...     0       1\n","39   7SKko21jIuhhaKhmLd8d  zwg96dRoh1PPCjucuXJYPxAKQlg1z  ...     0       1\n","\n","[134 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"ifWAhosLO58X","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import json\n","from pandas.io.json import json_normalize\n","import requests.api\n","import re\n","from IPython.display import clear_output, display\n","from os import system, name \n","\n","def unpack_path_values(path_values):\n","    users_list = []\n","    conversations_list = []\n","    messages_list = []\n","\n","    n = len(path_values)\n","\n","    #unpack hidden fields inside of path column\n","    for i, text in enumerate(path_values):\n","\n","        users_search = re.search(r'\"users\", \"(.*?)\"', text)\n","        conversations_search = re.search(r'\"conversations\", \"(.*?)\"', text)\n","        messages_search = re.search(r'\"messages\", \"(.*?)\"', text)\n","\n","        if not users_search is None:\n","            users = users_search.group(1)\n","        else:\n","            users = np.nan\n","\n","        if not conversations_search is None:\n","            conversations = conversations_search.group(1)\n","        else:\n","            conversations = np.nan\n","\n","        if not messages_search is None:\n","            messages = messages_search.group(1)\n","        else:\n","            messages = np.nan\n","\n","        users_list.append(users)\n","        conversations_list.append(conversations)\n","        messages_list.append(messages)\n","\n","        print(f\"rows to process: {n-i}\")\n","\n","    return users_list, conversations_list, messages_list\n","\n","def clean_message_files():\n","    for i in range(10):\n","        df = pd.read_csv(f\"Hallo_Data/messages/messages-{i}.csv\")\n","\n","        print(\"started unpacking values\")\n","        users, conversations, messages = unpack_path_values(df['path'].values)\n","        print(\"finished unpacking values\")\n","\n","        #add unpacked fields to dataframe\n","        df[\"users\"] = users\n","        df[\"conversation\"] = conversations\n","        df[\"message\"] = messages\n","\n","        df = df.drop(columns=[\"path\"])\n","\n","        print(\"started pickling\")\n","        #save results\n","        df.to_pickle(f\"Hallo_Data/messages_clean/messages-{i}.pkl\")\n","        print(\"finished pickling\")\n","\n","def stack_message_files():\n","    df_0 = pd.read_pickle(\"Hallo_Data/messages_clean/messages-0.pkl\")\n","\n","    for i in range(1, 10):\n","        print(f\"concatenating {i}\")\n","        df_1 = pd.read_pickle(f\"Hallo_Data/messages_clean/messages-{i}.pkl\")\n","        df_0 = pd.concat([df_0, df_1], ignore_index=True)\n","\n","    df_0.to_pickle(f\"Hallo_Data/messages_clean/messages.pkl\")\n","\n","\n","\n","stack_message_files()\n","\n","\n","\n","def format_data():\n","    with open(\"Hallo_Data/messages-0.json\", 'r+') as infile:\n","        text = infile.readlines()[-1]\n","        inner_text = re.findall(r'\\w+\\\\\"', text)\n","        users = inner_text[1][:-2]\n","        conversations = inner_text[3][:-2]\n","        messages = inner_text[5][:-2]\n","        # print(inner_text)\n","        # print(\"\\n\")\n","        # print(users, conversations, messages)\n","        # print(\"\\n\")\n","        # print(text)\n","        \n","        return\n","        text = '[' + \",\".join(text) + ']'\n","\n","    with open(\"Hallo_Data/test1.json\", 'r+') as infile:\n","        infile.write(text)\n","\n","def clean_message_data():\n","    df = pd.read_json(\"Hallo_Data/test1.json\")\n","    print(df[:3].values)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pxtN-cU2Byqt","colab_type":"code","colab":{}},"source":["m = pd.read_pickle(clean_folder+\"messages/messages-0.pkl\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vsAtHotxCBU8","colab_type":"code","outputId":"cdd60823-f8d7-4f11-cfa4-8f706f083f5a","executionInfo":{"status":"ok","timestamp":1575431483940,"user_tz":420,"elapsed":620,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["m.sample(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>messageType</th>\n","      <th>timestamp</th>\n","      <th>duration</th>\n","      <th>users</th>\n","      <th>conversation</th>\n","      <th>message</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4221578</th>\n","      <td>text</td>\n","      <td>1569082197055</td>\n","      <td>0</td>\n","      <td>4Q7pFbKCzLMofgQNd0gPDXxehO93</td>\n","      <td>4Q7pFbKCzLMofgQNd0gPDXxehO93_KcNHzpOIv2SvkrivK...</td>\n","      <td>VwcoFkCufkRUHqIPPiz4</td>\n","    </tr>\n","    <tr>\n","      <th>1277993</th>\n","      <td>text</td>\n","      <td>1572009561595</td>\n","      <td>0</td>\n","      <td>0pMgLQVmqpOoTSgRigtzMmA30QP2</td>\n","      <td>0pMgLQVmqpOoTSgRigtzMmA30QP2_QpBzzS05XqSAkkoJ6...</td>\n","      <td>rzNdjCcaUBbJPN7KH5jI</td>\n","    </tr>\n","    <tr>\n","      <th>4610201</th>\n","      <td>text</td>\n","      <td>1563449589490</td>\n","      <td>0</td>\n","      <td>Vz9QjmmuBqPQfKOOwUDPhVoFzx73</td>\n","      <td>Vz9QjmmuBqPQfKOOwUDPhVoFzx73_qVds125unCS4xngUI...</td>\n","      <td>CI8O2djhgZIYR6kjjIRd</td>\n","    </tr>\n","    <tr>\n","      <th>1900840</th>\n","      <td>text</td>\n","      <td>1567083540122</td>\n","      <td>0</td>\n","      <td>eNY4FFic16TS3b8hUyeOfXp4nw12</td>\n","      <td>3nlhYEb3RNPILo4Mj5swSwcuMLf2_eNY4FFic16TS3b8hU...</td>\n","      <td>LByIhWjV0bSo5ymp5T0N</td>\n","    </tr>\n","    <tr>\n","      <th>2731416</th>\n","      <td>text</td>\n","      <td>1570576100692</td>\n","      <td>0</td>\n","      <td>hjomZO0XXnXE8rAxIbhaHlte1Bp2</td>\n","      <td>NT6BwVwOoqNA9FGasi5gPzDt5pp2_hjomZO0XXnXE8rAxI...</td>\n","      <td>A71Z3JOG0RsyJxCJw4hu</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        messageType  ...               message\n","4221578        text  ...  VwcoFkCufkRUHqIPPiz4\n","1277993        text  ...  rzNdjCcaUBbJPN7KH5jI\n","4610201        text  ...  CI8O2djhgZIYR6kjjIRd\n","1900840        text  ...  LByIhWjV0bSo5ymp5T0N\n","2731416        text  ...  A71Z3JOG0RsyJxCJw4hu\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"Hh1krUplCJBM","colab_type":"code","outputId":"4302e893-2907-4711-bdbe-8a682b8ef63a","executionInfo":{"status":"ok","timestamp":1583019373824,"user_tz":420,"elapsed":7794,"user":{"displayName":"Cody Kesler","photoUrl":"","userId":"02599933710590207203"}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["pd.read_pickle(clean_folder+\"messages/messages-1.pkl\")"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>messageType</th>\n","      <th>timestamp</th>\n","      <th>duration</th>\n","      <th>users</th>\n","      <th>conversation</th>\n","      <th>message</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>text</td>\n","      <td>1563745896920</td>\n","      <td>0</td>\n","      <td>GhiR9zmhPnaV0wq655PxyZsVtAg2</td>\n","      <td>GhiR9zmhPnaV0wq655PxyZsVtAg2_fFWV566rxJdY4fm5q...</td>\n","      <td>mSMH0q6POFMj0hz6fR4D</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>text</td>\n","      <td>1563998851338</td>\n","      <td>0</td>\n","      <td>GhiR9zmhPnaV0wq655PxyZsVtAg2</td>\n","      <td>GHCWuZlKCmS7xNtBDY3Hg3yiRzl1_GhiR9zmhPnaV0wq65...</td>\n","      <td>U23yqKj4ktDWSdZ5IK0W</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>text</td>\n","      <td>1569668424140</td>\n","      <td>0</td>\n","      <td>S2Agtp6BKIOphYbOeLaCTnOuKuu1</td>\n","      <td>1BIplSsRJOdY6YyPiAg1wgQICed2_S2Agtp6BKIOphYbOe...</td>\n","      <td>0eXwTl1VgR8NAXfn7TW1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>text</td>\n","      <td>1569900194977</td>\n","      <td>0</td>\n","      <td>S2Agtp6BKIOphYbOeLaCTnOuKuu1</td>\n","      <td>IvUA0oUAzwYGkzJhGOnNObfzyId2_S2Agtp6BKIOphYbOe...</td>\n","      <td>dKRLinilMgJUINUxTeYJ</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>text</td>\n","      <td>1567328751439</td>\n","      <td>0</td>\n","      <td>tgHERuLkSVMS7Hu0mB1ayLyO2f12</td>\n","      <td>XvkzrSuX0sZlR40xuuydkSFQeeE3_tgHERuLkSVMS7Hu0m...</td>\n","      <td>Y0jyNcJtP4RTcHKmeFdG</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5096148</th>\n","      <td>text</td>\n","      <td>1561971980313</td>\n","      <td>0</td>\n","      <td>67fmXQbqDYQIJsXyYQvx99qQ2KI3</td>\n","      <td>67fmXQbqDYQIJsXyYQvx99qQ2KI3_JTnoRQSsSLWcVnfWR...</td>\n","      <td>pj4MfnsLabfO9W1JN0OI</td>\n","    </tr>\n","    <tr>\n","      <th>5096149</th>\n","      <td>text</td>\n","      <td>1571166068035</td>\n","      <td>0</td>\n","      <td>2ArS6jsm9YeJFxFbYPSbIQmKxT13</td>\n","      <td>2ArS6jsm9YeJFxFbYPSbIQmKxT13_JTnoRQSsSLWcVnfWR...</td>\n","      <td>qmYmv7uBWOQwUVDnKTZn</td>\n","    </tr>\n","    <tr>\n","      <th>5096150</th>\n","      <td>text</td>\n","      <td>1571394936625</td>\n","      <td>0</td>\n","      <td>JTnoRQSsSLWcVnfWRy52EucSf8T2</td>\n","      <td>8GlCpFcaaMaD9kVsKAKs4aIExls2_JTnoRQSsSLWcVnfWR...</td>\n","      <td>tAACZYv6YcFe5BbzmUTC</td>\n","    </tr>\n","    <tr>\n","      <th>5096151</th>\n","      <td>text</td>\n","      <td>1571146342540</td>\n","      <td>0</td>\n","      <td>lctF5xG4KlQAlk3fqO68D1h1lQn2</td>\n","      <td>JTnoRQSsSLWcVnfWRy52EucSf8T2_lctF5xG4KlQAlk3fq...</td>\n","      <td>i4YQw1MmGGas8UWNEUCA</td>\n","    </tr>\n","    <tr>\n","      <th>5096152</th>\n","      <td>text</td>\n","      <td>1571407273107</td>\n","      <td>0</td>\n","      <td>JTnoRQSsSLWcVnfWRy52EucSf8T2</td>\n","      <td>0pMgLQVmqpOoTSgRigtzMmA30QP2_JTnoRQSsSLWcVnfWR...</td>\n","      <td>evybWPPdd1T8IOYV985U</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5096153 rows √ó 6 columns</p>\n","</div>"],"text/plain":["        messageType  ...               message\n","0              text  ...  mSMH0q6POFMj0hz6fR4D\n","1              text  ...  U23yqKj4ktDWSdZ5IK0W\n","2              text  ...  0eXwTl1VgR8NAXfn7TW1\n","3              text  ...  dKRLinilMgJUINUxTeYJ\n","4              text  ...  Y0jyNcJtP4RTcHKmeFdG\n","...             ...  ...                   ...\n","5096148        text  ...  pj4MfnsLabfO9W1JN0OI\n","5096149        text  ...  qmYmv7uBWOQwUVDnKTZn\n","5096150        text  ...  tAACZYv6YcFe5BbzmUTC\n","5096151        text  ...  i4YQw1MmGGas8UWNEUCA\n","5096152        text  ...  evybWPPdd1T8IOYV985U\n","\n","[5096153 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"n-gNX0IjTlxO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}